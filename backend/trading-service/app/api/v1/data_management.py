"""
æ•°æ®ç®¡ç†APIæ¥å£
æä¾›å†å²æ•°æ®ä¸‹è½½ã€è´¨é‡ç›‘æ§ã€æ€§èƒ½åˆ†æçš„ç®¡ç†æ¥å£
"""

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, select, func
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import logging
import asyncio
import json

from app.database import get_db
from app.middleware.auth import get_current_active_user
from app.models.data_collection import DataCollectionTask
from app.services.historical_data_downloader import historical_data_downloader, data_sync_scheduler
from app.services.tick_data_manager import tick_data_manager, tick_to_kline_aggregator
from app.services.data_quality_monitor import data_quality_monitor, data_completeness_service
from app.services.okx_data_downloader import okx_data_downloader, DataType, TaskStatus, ResourceMonitor
from app.services.okx_data_downloader_enhanced import enhanced_okx_downloader, EnhancedDownloadTask
from app.services.ccxt_historical_downloader import CCXTHistoricalDownloader, CCXTDownloadTaskManager

router = APIRouter(prefix="/data", tags=["æ•°æ®ç®¡ç†"])
logger = logging.getLogger(__name__)

@router.get("/test")
async def test_route():
    """æµ‹è¯•è·¯ç”±æ˜¯å¦å·¥ä½œ"""
    return {"success": True, "message": "Data management API is working"}



@router.get("/timeframes")
async def get_available_timeframes(
    symbol: str,
    exchange: str = 'okx',
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æŒ‡å®šäº¤æ˜“å¯¹çš„å¯ç”¨æ—¶é—´æ¡†æ¶"""
    try:
        query = text("""
        SELECT 
            timeframe,
            MIN(timestamp) as start_date,
            MAX(timestamp) as end_date,
            COUNT(*) as record_count
        FROM market_data 
        WHERE exchange = :exchange AND symbol = :symbol
        GROUP BY timeframe
        ORDER BY 
            CASE timeframe 
                WHEN '1m' THEN 1 WHEN '5m' THEN 2 WHEN '15m' THEN 3 
                WHEN '30m' THEN 4 WHEN '1h' THEN 5 WHEN '2h' THEN 6 
                WHEN '4h' THEN 7 WHEN '1d' THEN 8 WHEN '1w' THEN 9 
                ELSE 99 END
        """)
        
        result = await db.execute(query, {"exchange": exchange, "symbol": symbol})
        rows = result.fetchall()
        
        timeframes_data = []
        for row in rows:
            timeframes_data.append({
                "timeframe": row.timeframe,
                "start_date": row.start_date.isoformat() if row.start_date else None,
                "end_date": row.end_date.isoformat() if row.end_date else None,
                "record_count": row.record_count
            })
        
        return {
            "success": True,
            "data": {
                "symbol": symbol,
                "exchange": exchange,
                "timeframes": timeframes_data
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ—¶é—´æ¡†æ¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/quality/check/{exchange}/{symbol}/{timeframe}")
async def check_data_quality(
    exchange: str,
    symbol: str,
    timeframe: str,
    check_days: int = 7,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    try:
        result = await data_quality_monitor.run_comprehensive_quality_check(
            exchange, symbol, timeframe, check_days, db
        )
        return {"success": True, "data": result}
        
    except Exception as e:
        logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/availability/{exchange}/{symbol}/{timeframe}")
async def check_data_availability(
    exchange: str,
    symbol: str,
    timeframe: str,
    start_date: str,
    end_date: str,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
    try:
        start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
        end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        
        availability = await historical_data_downloader.check_data_availability(
            exchange, symbol, timeframe, start_dt, end_dt, db
        )
        
        return {"success": True, "data": availability}
        
    except Exception as e:
        logger.error(f"æ•°æ®å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/download/historical")
async def download_historical_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """ä¸‹è½½å†å²æ•°æ®"""
    try:
        exchange = request.get('exchange')
        symbol = request.get('symbol')
        timeframe = request.get('timeframe')
        start_date = datetime.fromisoformat(request.get('start_date').replace('Z', '+00:00'))
        end_date = datetime.fromisoformat(request.get('end_date').replace('Z', '+00:00'))
        
        # åå°ä»»åŠ¡ä¸‹è½½
        background_tasks.add_task(
            _background_download_task,
            exchange, symbol, timeframe, start_date, end_date
        )
        
        return {
            "success": True,
            "message": f"å†å²æ•°æ®ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨: {exchange} {symbol} {timeframe}",
            "estimated_duration": "5-30åˆ†é’Ÿ"
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å†å²æ•°æ®ä¸‹è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/storage/stats")
async def get_storage_statistics(
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # Kçº¿æ•°æ®ç»Ÿè®¡
        kline_query = text("""
        SELECT 
            exchange,
            COUNT(DISTINCT symbol) as symbol_count,
            COUNT(DISTINCT timeframe) as timeframe_count,
            COUNT(*) as total_records
        FROM market_data
        GROUP BY exchange
        """)
        
        kline_result = await db.execute(kline_query)
        kline_stats = []
        
        for row in kline_result.fetchall():
            kline_stats.append({
                'exchange': row[0],
                'symbol_count': row[1],
                'timeframe_count': row[2],
                'total_records': row[3]
            })
        
        return {
            "success": True,
            "data": {
                "kline_statistics": kline_stats
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================
# åå°ä»»åŠ¡å‡½æ•°
# ================================

async def _background_download_task(
    exchange: str,
    symbol: str,
    timeframe: str,
    start_date: datetime,
    end_date: datetime
):
    """åå°ä¸‹è½½ä»»åŠ¡"""
    from app.database import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        try:
            result = await historical_data_downloader.download_historical_klines(
                exchange, symbol, timeframe, start_date, end_date, db
            )
            logger.info(f"åå°ä¸‹è½½å®Œæˆ: {result}")
        except Exception as e:
            logger.error(f"åå°ä¸‹è½½å¤±è´¥: {str(e)}")

async def _background_repair_task(
    exchange: str,
    symbol: str,
    timeframe: str
):
    """åå°æ•°æ®ä¿®å¤ä»»åŠ¡"""
    from app.database import AsyncSessionLocal
    async with AsyncSessionLocal() as db:
        try:
            result = await data_completeness_service.auto_repair_missing_data(
                exchange, symbol, timeframe, db
            )
            logger.info(f"æ•°æ®ä¿®å¤å®Œæˆ: {result}")
        except Exception as e:
            logger.error(f"æ•°æ®ä¿®å¤å¤±è´¥: {str(e)}")

async def _background_ccxt_download_task(
    task_id: str,
    exchange_id: str,
    symbols: List[str],
    timeframes: List[str],
    days_back: int
):
    """CCXTåå°ä¸‹è½½ä»»åŠ¡"""
    logger.info(f"ğŸš€ å¼€å§‹CCXTä¸‹è½½ä»»åŠ¡: {task_id}")
    try:
        # ä½¿ç”¨CCXTDownloadTaskManagerç®€åŒ–æ¥å£
        result = await CCXTDownloadTaskManager.download_okx_historical_data(
            symbols=symbols,
            timeframes=timeframes,
            days_back=days_back
        )
        
        logger.info(f"âœ… CCXTä¸‹è½½ä»»åŠ¡å®Œæˆ {task_id}: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ CCXTä¸‹è½½ä»»åŠ¡å¤±è´¥ {task_id}: {str(e)}")
        raise

async def _background_ccxt_bulk_download_task(
    task_id: str,
    exchange_id: str,
    symbols: List[str],
    timeframes: List[str],
    years_back: int
):
    """CCXTæ‰¹é‡åå°ä¸‹è½½ä»»åŠ¡"""
    logger.info(f"ğŸš€ å¼€å§‹CCXTæ‰¹é‡ä¸‹è½½ä»»åŠ¡: {task_id}")
    try:
        all_results = []
        total_symbols = len(symbols)
        
        # é€ä¸ªä¸‹è½½äº¤æ˜“å¯¹ä»¥é¿å…è¿‡è½½
        for i, symbol in enumerate(symbols):
            logger.info(f"ğŸ“Š æ‰¹é‡ä¸‹è½½è¿›åº¦: {i+1}/{total_symbols} - {symbol}")
            
            # ä½¿ç”¨CCXTDownloadTaskManagerçš„é•¿æœŸæ•°æ®ä¸‹è½½æ¥å£
            result = await CCXTDownloadTaskManager.download_long_term_data(
                symbol=symbol,
                timeframe=timeframes[0] if timeframes else '1h',  # é€‰æ‹©ç¬¬ä¸€ä¸ªæ—¶é—´æ¡†æ¶
                years_back=years_back,
                exchange_id=exchange_id
            )
            
            all_results.append({
                'symbol': symbol,
                'result': result
            })
            
            # ç»™å…¶ä»–ä»»åŠ¡è®©å‡ºCPUæ—¶é—´
            await asyncio.sleep(0.5)
        
        summary = {
            'total_symbols': total_symbols,
            'completed_symbols': len([r for r in all_results if r['result'].get('success', False)]),
            'failed_symbols': len([r for r in all_results if not r['result'].get('success', False)]),
            'results': all_results
        }
        
        logger.info(f"âœ… CCXTæ‰¹é‡ä¸‹è½½å®Œæˆ {task_id}: {summary}")
        return summary
        
    except Exception as e:
        logger.error(f"âŒ CCXTæ‰¹é‡ä¸‹è½½å¤±è´¥ {task_id}: {str(e)}")
        raise

async def _create_ccxt_task_record_async(
    task_id: str, 
    symbols: List[str],
    timeframes: List[str],
    start_date: str,
    end_date: str
):
    """åœ¨æ•°æ®åº“ä¸­åˆ›å»ºCCXTä¸‹è½½ä»»åŠ¡è®°å½• (ç‹¬ç«‹ä¼šè¯)"""
    from app.database import AsyncSessionLocal
    
    async with AsyncSessionLocal() as db:
        try:
            task_record = DataCollectionTask(
                task_name=f"CCXTå†å²æ•°æ®ä¸‹è½½_{task_id[-8:]}",
                exchange="okx",
                data_type="ccxt_kline",
                symbols=json.dumps(symbols),
                timeframes=json.dumps(timeframes),
                status="running",
                schedule_type="manual",
                config=json.dumps({
                    "task_id": task_id,
                    "start_date": start_date,
                    "end_date": end_date,
                    "method": "ccxt_fallback",
                    "description": "å†å²æ•°æ®CCXTæ™ºèƒ½ä¸‹è½½"
                })
            )
            
            db.add(task_record)
            await db.commit()
            await db.refresh(task_record)
            
            logger.info(f"âœ… CCXTä»»åŠ¡è®°å½•å·²åˆ›å»º: {task_id} (æ•°æ®åº“ID: {task_record.id})")
            return task_record.id
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºCCXTä»»åŠ¡è®°å½•å¤±è´¥ {task_id}: {str(e)}")
            await db.rollback()
            raise

async def _update_ccxt_task_status_async(
    task_id: str,
    status: str,
    result_data: Dict[str, Any] = None
):
    """æ›´æ–°CCXTä»»åŠ¡çŠ¶æ€ (ç‹¬ç«‹ä¼šè¯)"""
    from app.database import AsyncSessionLocal
    
    async with AsyncSessionLocal() as db:
        try:
            # é€šè¿‡configå­—æ®µä¸­çš„task_idæŸ¥æ‰¾ä»»åŠ¡
            stmt = select(DataCollectionTask).where(
                DataCollectionTask.config.like(f'%"task_id": "{task_id}"%')
            )
            result = await db.execute(stmt)
            task_record = result.scalar_one_or_none()
            
            if task_record:
                task_record.status = status
                task_record.updated_at = datetime.now()
                
                if status == "completed" and result_data:
                    task_record.success_count = task_record.success_count + 1
                    task_record.total_records = result_data.get('total_records_downloaded', result_data.get('total_records', 0))
                elif status == "failed":
                    task_record.error_count = task_record.error_count + 1
                    if result_data and result_data.get('error'):
                        task_record.last_error_message = str(result_data['error'])
                        task_record.last_error_at = datetime.now()
                
                await db.commit()
                logger.info(f"âœ… CCXTä»»åŠ¡çŠ¶æ€å·²æ›´æ–°: {task_id} -> {status}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°CCXTä»»åŠ¡è®°å½•: {task_id}")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°CCXTä»»åŠ¡çŠ¶æ€å¤±è´¥ {task_id}: {str(e)}")
            await db.rollback()

async def _background_ccxt_fallback_task(
    task_id: str,
    exchange_id: str,
    ccxt_symbols: List[str],
    timeframes: List[str],
    days_back: int
):
    """CCXTå›é€€ä»»åŠ¡ - å½“OKX APIå¤±è´¥æ—¶çš„æ™ºèƒ½å›é€€"""
    logger.info(f"ğŸ”„ å¼€å§‹CCXTå›é€€ä»»åŠ¡: {task_id} ({days_back}å¤©å†å²æ•°æ®)")
    
    try:
        result = await CCXTDownloadTaskManager.download_okx_historical_data(
            symbols=ccxt_symbols,
            timeframes=timeframes,
            days_back=days_back
        )
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        await _update_ccxt_task_status_async(task_id, "completed", result)
        
        logger.info(f"âœ… CCXTå›é€€ä»»åŠ¡å®Œæˆ {task_id}: {result}")
        return result
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        await _update_ccxt_task_status_async(task_id, "failed", {"error": str(e)})
        
        logger.error(f"âŒ CCXTå›é€€ä»»åŠ¡å¤±è´¥ {task_id}: {str(e)}")
        raise

# ================================
# OKXæ•°æ®ä¸‹è½½API (æ–°å¢çœŸå®ä¸‹è½½åŠŸèƒ½)
# ================================

@router.get("/system/resources")
async def get_system_resources(
    current_user = Depends(get_current_active_user)
):
    """è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€"""
    try:
        monitor = ResourceMonitor()
        available, message = monitor.is_resource_available()
        
        return {
            "success": True,
            "data": {
                "resources_available": available,
                "status_message": message,
                "memory_usage_percent": monitor.get_memory_usage(),
                "cpu_usage_percent": monitor.get_cpu_usage(),
                "process_memory_mb": monitor.get_process_memory_mb(),
                "limits": {
                    "max_memory_percent": monitor.max_memory_percent,
                    "max_cpu_percent": monitor.max_cpu_percent
                }
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/okx/tick/download")
async def download_okx_tick_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """ä¸‹è½½OKX Tickæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ™ºèƒ½åˆ†æ‰¹å¤„ç†"""
    try:
        symbols = request.get('symbols', ['BTC'])
        start_date = request.get('start_date')  # æ ¼å¼: 20240101
        end_date = request.get('end_date')      # æ ¼å¼: 20240131
        
        if not all([symbols, start_date, end_date]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: symbols, start_date, end_date")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶æ€
        monitor = ResourceMonitor()
        available, resource_message = monitor.is_resource_available()
        
        if not available:
            raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œæ— æ³•å¯åŠ¨ä¸‹è½½ä»»åŠ¡: {resource_message}")
        
        # æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡æ•°é‡
        active_tasks = await okx_data_downloader.list_active_tasks()
        running_tasks = [task for task in active_tasks if task.status == TaskStatus.RUNNING]
        
        if len(running_tasks) >= 1:  # é™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°
            raise HTTPException(
                status_code=429, 
                detail=f"å·²æœ‰ {len(running_tasks)} ä¸ªä¸‹è½½ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·ç¨åå†è¯•ã€‚æç¤ºï¼šå»ºè®®ç­‰å¾…å½“å‰ä»»åŠ¡å®Œæˆæˆ–ä½¿ç”¨æ›´çŸ­çš„æ—¶é—´èŒƒå›´"
            )
        
        # æ£€æŸ¥æ—¶é—´èŒƒå›´åˆç†æ€§
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        days_span = (end_dt - start_dt).days + 1
        total_files_estimate = len(symbols) * days_span
        
        # æä¾›æ™ºèƒ½å»ºè®®
        optimization_tips = []
        if total_files_estimate > 10:
            optimization_tips.append(f"å»ºè®®åˆ†æ‰¹ä¸‹è½½ï¼šå½“å‰éœ€ä¸‹è½½ {total_files_estimate} ä¸ªæ–‡ä»¶")
            # è‡ªåŠ¨è°ƒæ•´åˆ°åˆç†èŒƒå›´
            max_days = 10 // len(symbols)
            if max_days < 1:
                max_days = 1
            suggested_end = start_dt + timedelta(days=max_days-1)
            if suggested_end < end_dt:
                optimization_tips.append(f"æœ¬æ¬¡ä»»åŠ¡å°†è‡ªåŠ¨é™åˆ¶åˆ° {suggested_end.strftime('%Y%m%d')}")
        
        if days_span > 7:
            optimization_tips.append("å»ºè®®æ¯æ¬¡ä¸‹è½½ä¸è¶…è¿‡7å¤©æ•°æ®ï¼Œä»¥æé«˜æˆåŠŸç‡")
            
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡ (å†…éƒ¨ä¼šè‡ªåŠ¨é™åˆ¶æ–‡ä»¶æ•°é‡)
        task = await okx_data_downloader.create_tick_download_task(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date
        )
        
        # åå°æ‰§è¡Œä»»åŠ¡
        background_tasks.add_task(
            okx_data_downloader.execute_tick_download_task,
            task.task_id
        )
        
        return {
            "success": True,
            "data": {
                "task_id": task.task_id,
                "data_type": "tick",
                "symbols": symbols,
                "date_range": f"{start_date} - {end_date}",
                "total_files": task.total_files,
                "message": "OKX Tickæ•°æ®ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨ (æ™ºèƒ½ä¼˜åŒ–ç‰ˆ)",
                "optimization_applied": {
                    "file_limit": task.total_files <= 10,
                    "batch_processing": True,
                    "memory_optimization": True,
                    "resource_monitoring": True
                },
                "tips": optimization_tips if optimization_tips else ["ä»»åŠ¡é…ç½®åˆç†ï¼Œæ— éœ€é¢å¤–ä¼˜åŒ–"],
                "resource_status": {
                    "memory_usage": f"{monitor.get_memory_usage():.1f}%",
                    "cpu_usage": f"{monitor.get_cpu_usage():.1f}%",
                    "process_memory": f"{monitor.get_process_memory_mb():.1f}MB"
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨OKX Tickæ•°æ®ä¸‹è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/okx/tick/download-quick")
async def download_okx_tick_data_quick(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """å¿«é€Ÿä¸‹è½½OKX Tickæ•°æ® - é™åˆ¶3å¤©å†…ï¼Œå•äº¤æ˜“å¯¹"""
    try:
        symbol = request.get('symbol', 'BTC')  # å•ä¸ªäº¤æ˜“å¯¹
        start_date = request.get('start_date')  # æ ¼å¼: 20240101
        days = min(request.get('days', 1), 3)  # æœ€å¤š3å¤©
        
        if not start_date:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: start_date")
        
        # è®¡ç®—ç»“æŸæ—¥æœŸ
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = start_dt + timedelta(days=days-1)
        end_date = end_dt.strftime('%Y%m%d')
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        monitor = ResourceMonitor()
        available, resource_message = monitor.is_resource_available()
        
        if not available:
            raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿèµ„æºä¸è¶³: {resource_message}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
        active_tasks = await okx_data_downloader.list_active_tasks()
        running_tasks = [task for task in active_tasks if task.status == TaskStatus.RUNNING]
        
        if len(running_tasks) >= 1:
            raise HTTPException(
                status_code=429, 
                detail="æœ‰å…¶ä»–ä¸‹è½½ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œå¿«é€Ÿä¸‹è½½åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
            )
        
        # åˆ›å»ºå¿«é€Ÿä¸‹è½½ä»»åŠ¡
        task = await okx_data_downloader.create_tick_download_task(
            symbols=[symbol],
            start_date=start_date,
            end_date=end_date
        )
        
        # åå°æ‰§è¡Œä»»åŠ¡
        background_tasks.add_task(
            okx_data_downloader.execute_tick_download_task,
            task.task_id
        )
        
        return {
            "success": True,
            "data": {
                "task_id": task.task_id,
                "data_type": "tick_quick",
                "symbol": symbol,
                "date_range": f"{start_date} - {end_date}",
                "days": days,
                "total_files": task.total_files,
                "message": f"å¿«é€ŸTickæ•°æ®ä¸‹è½½å·²å¯åŠ¨ ({symbol}, {days}å¤©)",
                "estimated_time": f"{task.total_files * 30}ç§’ - {task.total_files * 60}ç§’",
                "features": [
                    "å•äº¤æ˜“å¯¹å¿«é€Ÿä¸‹è½½",
                    "æœ€å¤š3å¤©æ•°æ®",
                    "ä¼˜å…ˆçº§å¤„ç†",
                    "èµ„æºä¼˜åŒ–"
                ]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¿«é€Ÿä¸‹è½½å¯åŠ¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/okx/tick/download-batch")
async def download_okx_tick_data_batch(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """OKX Tickæ•°æ®æ‰¹é‡ä¸‹è½½ - æŒ‰æ—¥ç»´åº¦è‡ªåŠ¨åˆ†å‰²ä»»åŠ¡"""
    try:
        symbols = request.get('symbols', ['BTC'])
        start_date = request.get('start_date')  # æ ¼å¼: 20240701
        end_date = request.get('end_date')      # æ ¼å¼: 20240730
        max_concurrent = request.get('max_concurrent', 3)  # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
        
        if not all([symbols, start_date, end_date]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: symbols, start_date, end_date")
        
        # è§£ææ—¥æœŸèŒƒå›´
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        days_span = (end_dt - start_dt).days + 1
        
        if days_span <= 0:
            raise HTTPException(status_code=400, detail="ç»“æŸæ—¥æœŸå¿…é¡»å¤§äºæˆ–ç­‰äºå¼€å§‹æ—¥æœŸ")
        
        if days_span > 31:
            raise HTTPException(status_code=400, detail="å•æ¬¡æ‰¹é‡ä¸‹è½½ä¸èƒ½è¶…è¿‡31å¤©ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶æ€
        monitor = ResourceMonitor()
        available, resource_message = monitor.is_resource_available()
        
        if not available:
            raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿèµ„æºä¸è¶³: {resource_message}")
        
        # æ£€æŸ¥å½“å‰è¿è¡Œçš„ä»»åŠ¡
        active_tasks = await okx_data_downloader.list_active_tasks()
        running_tasks = [task for task in active_tasks if task.status == TaskStatus.RUNNING]
        
        if len(running_tasks) >= max_concurrent:
            raise HTTPException(
                status_code=429, 
                detail=f"å½“å‰æœ‰ {len(running_tasks)} ä¸ªä»»åŠ¡åœ¨è¿è¡Œï¼Œè¶…è¿‡æœ€å¤§å¹¶å‘æ•° {max_concurrent}"
            )
        
        # æŒ‰æ—¥æœŸåˆ†å‰²ä»»åŠ¡
        batch_tasks = []
        current_dt = start_dt
        task_count = 0
        
        while current_dt <= end_dt:
            date_str = current_dt.strftime('%Y%m%d')
            
            # ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹å’Œæ¯ä¸ªæ—¥æœŸåˆ›å»ºç‹¬ç«‹ä»»åŠ¡
            for symbol in symbols:
                # åˆ›å»ºå•æ—¥ä»»åŠ¡
                task = await okx_data_downloader.create_tick_download_task(
                    symbols=[symbol],
                    start_date=date_str,
                    end_date=date_str
                )
                
                batch_tasks.append({
                    "task_id": task.task_id,
                    "symbol": symbol,
                    "date": date_str,
                    "status": "pending",
                    "estimated_files": 1
                })
                
                task_count += 1
                
                # æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…åˆ›å»ºå¤ªå¤šä»»åŠ¡
                if task_count >= max_concurrent:
                    break
            
            if task_count >= max_concurrent:
                break
                
            current_dt += timedelta(days=1)
        
        # å¯åŠ¨å‰å‡ ä¸ªä»»åŠ¡ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
        started_tasks = 0
        for task_info in batch_tasks[:max_concurrent]:
            background_tasks.add_task(
                okx_data_downloader.execute_tick_download_task,
                task_info["task_id"]
            )
            task_info["status"] = "running"
            started_tasks += 1
        
        # è®¡ç®—ä»»åŠ¡ç»Ÿè®¡
        total_files_estimate = len(symbols) * days_span
        estimated_time_minutes = days_span * len(symbols) * 2  # æ¯ä¸ªæ–‡ä»¶å¤§çº¦2åˆ†é’Ÿ
        
        return {
            "success": True,
            "data": {
                "batch_id": f"batch_{int(datetime.now().timestamp())}",
                "batch_type": "tick_daily_split",
                "date_range": f"{start_date} - {end_date}",
                "symbols": symbols,
                "total_days": days_span,
                "total_tasks_created": len(batch_tasks),
                "tasks_started": started_tasks,
                "tasks_pending": len(batch_tasks) - started_tasks,
                "max_concurrent": max_concurrent,
                "estimated_files": total_files_estimate,
                "estimated_time": f"{estimated_time_minutes} åˆ†é’Ÿ",
                "tasks": batch_tasks,
                "message": f"å·²åˆ›å»º {len(batch_tasks)} ä¸ªæ—¥ç»´åº¦ä¸‹è½½ä»»åŠ¡ï¼Œå¯åŠ¨äº† {started_tasks} ä¸ªä»»åŠ¡",
                "recommendations": [
                    "ä»»åŠ¡å°†æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ†å‰²æ‰§è¡Œ",
                    "æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹è·Ÿè¸ªçŠ¶æ€",
                    "å¤±è´¥çš„ä»»åŠ¡ä¸å½±å“å…¶ä»–æ—¥æœŸ",
                    f"å»ºè®®æ¯ {max_concurrent * 2} åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡è¿›åº¦"
                ],
                "resource_status": {
                    "memory_usage": f"{monitor.get_memory_usage():.1f}%",
                    "cpu_usage": f"{monitor.get_cpu_usage():.1f}%",
                    "available": available
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡ä¸‹è½½å¯åŠ¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/okx/batch/status/{date_range}")
async def get_batch_download_status(
    date_range: str,  # æ ¼å¼: 20240701-20240730
    symbols: str = Query(default="BTC", description="äº¤æ˜“å¯¹ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”"),
    current_user = Depends(get_current_active_user)
):
    """æŸ¥è¯¢æ‰¹é‡ä¸‹è½½ä»»åŠ¡çŠ¶æ€ - æŒ‰æ—¥æœŸèŒƒå›´æ±‡æ€»æ‰€æœ‰ç›¸å…³ä»»åŠ¡"""
    try:
        # è§£ææ—¥æœŸèŒƒå›´
        if '-' not in date_range:
            raise HTTPException(status_code=400, detail="æ—¥æœŸèŒƒå›´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º: 20240701-20240730")
            
        start_date, end_date = date_range.split('-')
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        
        # è§£æäº¤æ˜“å¯¹
        symbol_list = [s.strip().upper() for s in symbols.split(',')]
        
        # è·å–æ‰€æœ‰æ´»è·ƒä»»åŠ¡
        all_tasks = await okx_data_downloader.list_active_tasks()
        
        # ç­›é€‰åŒ¹é…çš„ä»»åŠ¡ï¼ˆæŒ‰æ—¥æœŸå’Œäº¤æ˜“å¯¹ï¼‰
        matching_tasks = []
        current_dt = start_dt
        
        while current_dt <= end_dt:
            date_str = current_dt.strftime('%Y%m%d')
            
            for symbol in symbol_list:
                # æŸ¥æ‰¾åŒ¹é…è¿™ä¸ªæ—¥æœŸå’Œäº¤æ˜“å¯¹çš„ä»»åŠ¡
                for task in all_tasks:
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åŒ¹é…å½“å‰æ—¥æœŸå’Œäº¤æ˜“å¯¹
                    if (hasattr(task, 'task_id') and 
                        date_str in task.task_id and 
                        any(symbol.lower() in task.task_id.lower() for symbol in [symbol])):
                        
                        matching_tasks.append({
                            "task_id": task.task_id,
                            "symbol": symbol,
                            "date": date_str,
                            "status": task.status.value if hasattr(task.status, 'value') else str(task.status),
                            "progress": task.progress,
                            "downloaded_records": task.downloaded_records,
                            "total_files": task.total_files,
                            "processed_files": task.processed_files,
                            "created_at": task.created_at.isoformat() if task.created_at else None,
                            "started_at": task.started_at.isoformat() if task.started_at else None,
                            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                            "error_message": task.error_message or ""
                        })
            
            current_dt += timedelta(days=1)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_tasks = len(matching_tasks)
        completed_tasks = len([t for t in matching_tasks if t["status"] == "completed"])
        running_tasks = len([t for t in matching_tasks if t["status"] == "running"])
        failed_tasks = len([t for t in matching_tasks if t["status"] == "failed"])
        pending_tasks = len([t for t in matching_tasks if t["status"] == "pending"])
        
        total_records = sum(t["downloaded_records"] for t in matching_tasks)
        overall_progress = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
        daily_stats = {}
        current_dt = start_dt
        while current_dt <= end_dt:
            date_str = current_dt.strftime('%Y%m%d')
            date_tasks = [t for t in matching_tasks if t["date"] == date_str]
            
            daily_stats[date_str] = {
                "date": date_str,
                "total_symbols": len(symbol_list),
                "completed_symbols": len([t for t in date_tasks if t["status"] == "completed"]),
                "running_symbols": len([t for t in date_tasks if t["status"] == "running"]),
                "failed_symbols": len([t for t in date_tasks if t["status"] == "failed"]),
                "total_records": sum(t["downloaded_records"] for t in date_tasks),
                "tasks": date_tasks
            }
            
            current_dt += timedelta(days=1)
        
        return {
            "success": True,
            "data": {
                "batch_info": {
                    "date_range": date_range,
                    "symbols": symbol_list,
                    "total_days": (end_dt - start_dt).days + 1
                },
                "summary": {
                    "total_tasks": total_tasks,
                    "completed_tasks": completed_tasks,
                    "running_tasks": running_tasks,
                    "failed_tasks": failed_tasks,
                    "pending_tasks": pending_tasks,
                    "overall_progress": round(overall_progress, 2),
                    "total_downloaded_records": total_records
                },
                "daily_breakdown": daily_stats,
                "all_tasks": matching_tasks,
                "recommendations": [
                    f"æ•´ä½“è¿›åº¦: {completed_tasks}/{total_tasks} ä»»åŠ¡å®Œæˆ",
                    f"å·²ä¸‹è½½ {total_records:,} æ¡è®°å½•" if total_records > 0 else "æš‚æ— æ•°æ®ä¸‹è½½å®Œæˆ",
                    "å¤±è´¥çš„ä»»åŠ¡å¯ä»¥å•ç‹¬é‡æ–°æ‰§è¡Œ" if failed_tasks > 0 else "æ‰€æœ‰ä»»åŠ¡è¿è¡Œæ­£å¸¸",
                    "å»ºè®®ç­‰å¾…è¿è¡Œä¸­çš„ä»»åŠ¡å®Œæˆ" if running_tasks > 0 else "å½“å‰æ²¡æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡"
                ]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ‰¹é‡ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/okx/kline/download")
async def download_okx_kline_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """ä¸‹è½½OKX Kçº¿æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒèµ„æºæ£€æŸ¥"""
    try:
        symbols = request.get('symbols', ['BTC/USDT'])
        timeframes = request.get('timeframes', ['1h'])
        start_date = request.get('start_date')  # æ ¼å¼: 20240101
        end_date = request.get('end_date')      # æ ¼å¼: 20240131
        use_enhanced = request.get('use_enhanced', True)  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆä¸‹è½½å™¨
        
        if not all([symbols, timeframes, start_date, end_date]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: symbols, timeframes, start_date, end_date")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºçŠ¶æ€
        monitor = ResourceMonitor()
        available, resource_message = monitor.is_resource_available()
        
        if not available:
            raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œæ— æ³•å¯åŠ¨ä¸‹è½½ä»»åŠ¡: {resource_message}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ä¸‹è½½ä»»åŠ¡æ­£åœ¨è¿è¡Œ
        active_tasks = await okx_data_downloader.list_active_tasks()
        running_tasks = [task for task in active_tasks if task.status == TaskStatus.RUNNING]
        
        if len(running_tasks) >= 1:  # é™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°
            raise HTTPException(
                status_code=429, 
                detail=f"å·²æœ‰ {len(running_tasks)} ä¸ªä¸‹è½½ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·ç¨åå†è¯•"
            )
        
        if use_enhanced:
            # ğŸ†• æ™ºèƒ½é€‰æ‹©ï¼šå¯¹äºå†å²æ•°æ®ä½¿ç”¨CCXTï¼Œå¯¹äºå®æ—¶æ•°æ®ä½¿ç”¨å¢å¼ºç‰ˆ
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            current_time = datetime.now()
            days_ago = (current_time - start_dt).days
            
            if days_ago > 7:  # 7å¤©å‰çš„æ•°æ®ç”¨CCXTä¸‹è½½
                logger.info(f"ğŸ”„ å†å²æ•°æ® ({days_ago}å¤©å‰)ï¼Œä½¿ç”¨CCXTä¸‹è½½å™¨")
                
                # è½¬æ¢ä¸ºCCXTæ ¼å¼
                ccxt_symbols = []
                for symbol in symbols:
                    if 'USDT-SWAP' in symbol:
                        ccxt_symbols.append(symbol.replace('-USDT-SWAP', '/USDT:USDT'))
                    else:
                        ccxt_symbols.append(symbol.replace('-', '/'))
                
                # åˆ›å»ºCCXTä»»åŠ¡
                task_id = f"ccxt_fallback_okx_{'-'.join(symbols[0].split('-'))[:10]}_{int(datetime.now().timestamp())}"
                
                # åœ¨æ•°æ®åº“ä¸­åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆä½¿ç”¨ç‹¬ç«‹ä¼šè¯é¿å…å†²çªï¼‰
                await _create_ccxt_task_record_async(task_id, symbols, timeframes, start_date, end_date)
                
                background_tasks.add_task(
                    _background_ccxt_fallback_task,
                    task_id, 'okx', ccxt_symbols, timeframes, days_ago
                )
                
                return {
                    "success": True,
                    "data": {
                        "task_id": task_id,
                        "data_type": "ccxt_fallback",
                        "symbols": symbols,
                        "timeframes": timeframes,
                        "date_range": f"{start_date} - {end_date}",
                        "message": f"å†å²æ•°æ®ä½¿ç”¨CCXTä¸‹è½½å™¨ ({days_ago}å¤©å‰)",
                        "method": "ccxt_fallback",
                        "features": [
                            "è‡ªåŠ¨åˆ‡æ¢åˆ°CCXTè§£å†³å†å²æ•°æ®é—®é¢˜",
                            "æ™ºèƒ½ç«¯ç‚¹é€‰æ‹©",
                            "ç”Ÿäº§çº§é”™è¯¯å¤„ç†"
                        ]
                    }
                }
            else:
                # ä½¿ç”¨å¢å¼ºç‰ˆä¸‹è½½å™¨å¤„ç†è¿‘æœŸæ•°æ®
                task = await enhanced_okx_downloader.create_enhanced_kline_task(
                    symbols=symbols,
                    timeframes=timeframes,
                    start_date=start_date,
                    end_date=end_date
                )
                
                # åå°æ‰§è¡Œå¢å¼ºä»»åŠ¡
                background_tasks.add_task(
                    enhanced_okx_downloader.execute_enhanced_kline_task,
                    task.task_id
                )
            
            return {
                "success": True,
                "data": {
                    "task_id": task.task_id,
                    "data_type": "enhanced_kline",
                    "symbols": symbols,
                    "timeframes": timeframes,
                    "date_range": f"{start_date} - {end_date}",
                    "total_files": task.total_files,
                    "expected_records": task.expected_records,
                    "subtasks_count": len(task.subtasks),
                    "message": "OKX å¢å¼ºKçº¿æ•°æ®ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨",
                    "features": ["æ•°æ®å®Œæ•´æ€§éªŒè¯", "æ™ºèƒ½é‡è¯•æœºåˆ¶", "è´¨é‡è¯„åˆ†", "åˆ†é˜¶æ®µä¸‹è½½"],
                    "resource_status": {
                        "memory_usage": f"{monitor.get_memory_usage():.1f}%",
                        "cpu_usage": f"{monitor.get_cpu_usage():.1f}%",
                        "process_memory": f"{monitor.get_process_memory_mb():.1f}MB"
                    }
                }
            }
        else:
            # ä½¿ç”¨åŸç‰ˆä¸‹è½½å™¨
            task = await okx_data_downloader.create_kline_download_task(
                symbols=symbols,
                timeframes=timeframes,
                start_date=start_date,
                end_date=end_date
            )
            
            # åå°æ‰§è¡Œä»»åŠ¡
            background_tasks.add_task(
                okx_data_downloader.execute_kline_download_task,
                task.task_id
            )
            
            return {
                "success": True,
                "data": {
                    "task_id": task.task_id,
                    "data_type": "kline",
                    "symbols": symbols,
                    "timeframes": timeframes,
                    "date_range": f"{start_date} - {end_date}",
                    "total_files": task.total_files,
                    "message": "OKX Kçº¿æ•°æ®ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨",
                    "resource_status": {
                        "memory_usage": f"{monitor.get_memory_usage():.1f}%",
                        "cpu_usage": f"{monitor.get_cpu_usage():.1f}%",
                        "process_memory": f"{monitor.get_process_memory_mb():.1f}MB"
                    }
                }
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨OKX Kçº¿æ•°æ®ä¸‹è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

# ================================
# CCXTå†å²æ•°æ®ä¸‹è½½API (æ–°å¢)
# ================================

@router.post("/ccxt/download")
async def download_ccxt_historical_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user)
):
    """ä½¿ç”¨CCXTä¸‹è½½å†å²æ•°æ® - æ”¯æŒ1-2å¹´é•¿æœŸæ•°æ®"""
    try:
        exchange_id = request.get('exchange', 'okx')
        symbols = request.get('symbols', ['BTC/USDT:USDT'])
        timeframes = request.get('timeframes', ['1h'])
        days_back = request.get('days_back', 30)
        
        # è¾“å…¥éªŒè¯
        if not all([symbols, timeframes]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: symbols, timeframes")
        
        if days_back > 730:  # é™åˆ¶æœ€å¤š2å¹´
            raise HTTPException(status_code=400, detail="days_backä¸èƒ½è¶…è¿‡730å¤©")
        
        # åˆ›å»ºä»»åŠ¡ID
        task_id = f"ccxt_{exchange_id}_{'-'.join(symbols[0].split('/'))[:10]}_{int(datetime.now().timestamp())}"
        
        # å¯åŠ¨åå°ä»»åŠ¡
        background_tasks.add_task(
            _background_ccxt_download_task,
            task_id, exchange_id, symbols, timeframes, days_back
        )
        
        return {
            "success": True,
            "data": {
                "task_id": task_id,
                "exchange": exchange_id,
                "symbols": symbols,
                "timeframes": timeframes,
                "days_back": days_back,
                "message": f"CCXTå†å²æ•°æ®ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨ ({days_back}å¤©æ•°æ®)",
                "estimated_duration": f"{days_back // 30 * 5}-{days_back // 30 * 15}åˆ†é’Ÿ",
                "features": [
                    "æ”¯æŒ1-2å¹´å†å²æ•°æ®", 
                    "è‡ªåŠ¨ç«¯ç‚¹é€‰æ‹©(Candles/HistoryCandles)",
                    "æ™ºèƒ½åˆ†é¡µå’Œé‡è¯•æœºåˆ¶",
                    "ç”Ÿäº§çº§é”™è¯¯å¤„ç†"
                ]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨CCXTä¸‹è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.post("/ccxt/download/bulk")
async def download_ccxt_bulk_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user)
):
    """æ‰¹é‡ä¸‹è½½å¤šäº¤æ˜“å¯¹é•¿æœŸå†å²æ•°æ®"""
    try:
        exchange_id = request.get('exchange', 'okx')
        symbols = request.get('symbols', [])
        timeframes = request.get('timeframes', ['1h'])
        years_back = request.get('years_back', 1)
        
        if not symbols:
            raise HTTPException(status_code=400, detail="symbolsåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if years_back > 2:
            raise HTTPException(status_code=400, detail="years_backä¸èƒ½è¶…è¿‡2å¹´")
        
        # åˆ›å»ºæ‰¹é‡ä»»åŠ¡ID
        task_id = f"ccxt_bulk_{exchange_id}_{len(symbols)}symbols_{int(datetime.now().timestamp())}"
        
        # å¯åŠ¨æ‰¹é‡åå°ä»»åŠ¡
        background_tasks.add_task(
            _background_ccxt_bulk_download_task,
            task_id, exchange_id, symbols, timeframes, years_back
        )
        
        estimated_records = len(symbols) * len(timeframes) * years_back * 365 * 24  # ç²—ç•¥ä¼°ç®—
        
        return {
            "success": True,
            "data": {
                "task_id": task_id,
                "exchange": exchange_id,
                "symbols": symbols,
                "timeframes": timeframes,
                "years_back": years_back,
                "estimated_records": estimated_records,
                "message": f"æ‰¹é‡CCXTä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨ ({len(symbols)}ä¸ªäº¤æ˜“å¯¹, {years_back}å¹´æ•°æ®)",
                "estimated_duration": f"{years_back * len(symbols) * 10}-{years_back * len(symbols) * 30}åˆ†é’Ÿ"
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯åŠ¨æ‰¹é‡CCXTä¸‹è½½å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/okx/tasks")
async def list_okx_download_tasks(
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–OKXä¸‹è½½ä»»åŠ¡åˆ—è¡¨"""
    try:
        # ä»æ•°æ®åº“è·å–OKXä»»åŠ¡ï¼ˆæœ€è¿‘50æ¡ï¼‰
        query = text("""
        SELECT task_name, exchange, data_type, symbols, timeframes, status, 
               success_count, error_count, total_records, config, created_at,
               last_run_at, last_error_message
        FROM data_collection_tasks 
        WHERE exchange = 'okx' 
        ORDER BY created_at DESC 
        LIMIT 50
        """)
        
        result = await db.execute(query)
        task_list = []
        
        for row in result.fetchall():
            # è§£æé…ç½®ä¿¡æ¯
            import json
            config = json.loads(row[9]) if row[9] else {}
            
            # è®¡ç®—è¿›åº¦
            progress = 100.0 if row[5] == 'completed' else (
                50.0 if row[5] == 'running' else 0.0
            )
            
            task_info = {
                "task_id": row[0],  # task_name
                "data_type": row[2],  # data_type
                "exchange": row[1],   # exchange
                "symbols": [row[3]] if row[3] else [],  # symbols (stored as string)
                "date_range": f"{config.get('start_date', 'N/A')} - {config.get('end_date', 'N/A')}",
                "status": row[5],     # status
                "progress": progress,
                "total_files": config.get('total_files', 1),
                "processed_files": row[6],  # success_count
                "downloaded_records": row[8],  # total_records
                "created_at": row[10],  # created_at
                "started_at": row[11],  # last_run_at
                "completed_at": row[11] if row[5] == 'completed' else None,
                "error_message": row[12]  # last_error_message
            }
            
            # æ·»åŠ æ—¶é—´æ¡†æ¶ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯Kçº¿æ•°æ®ï¼‰
            if row[2] == 'kline' and row[4]:  # data_type == 'kline' and timeframes
                task_info["timeframes"] = [row[4]]  # timeframes stored as string
            
            task_list.append(task_info)
        
        return {
            "success": True,
            "data": task_list,
            "total_tasks": len(task_list)
        }
        
    except Exception as e:
        logger.error(f"è·å–OKXä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/okx/tasks/{task_id}")
async def get_okx_task_status(
    task_id: str,
    current_user = Depends(get_current_active_user)
):
    """è·å–OKXä¸‹è½½ä»»åŠ¡çŠ¶æ€"""
    try:
        # å…ˆå°è¯•å¢å¼ºç‰ˆä¸‹è½½å™¨
        enhanced_task_status = await enhanced_okx_downloader.get_enhanced_task_status(task_id)
        
        if enhanced_task_status:
            return {
                "success": True,
                "data": enhanced_task_status,
                "enhanced": True
            }
        
        # å›é€€åˆ°åŸç‰ˆä¸‹è½½å™¨
        task = await okx_data_downloader.get_task_status(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "data": {
                "task_id": task.task_id,
                "data_type": task.data_type.value,
                "exchange": task.exchange,
                "symbols": task.symbols,
                "status": task.status.value,
                "progress": round(task.progress, 1),
                "total_files": task.total_files,
                "processed_files": task.processed_files,
                "downloaded_records": task.downloaded_records,
                "date_range": f"{task.start_date} - {task.end_date}",
                "created_at": task.created_at.isoformat() if task.created_at else None,
                "started_at": task.started_at.isoformat() if task.started_at else None,
                "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                "error_message": task.error_message,
                "estimated_completion": _estimate_completion_time(task) if task.status == TaskStatus.RUNNING else None
            },
            "enhanced": False
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/okx/tasks/{task_id}")
async def cancel_okx_task(
    task_id: str,
    current_user = Depends(get_current_active_user)
):
    """å–æ¶ˆOKXä¸‹è½½ä»»åŠ¡"""
    try:
        success = await okx_data_downloader.cancel_task(task_id)
        
        if success:
            return {
                "success": True,
                "message": f"ä»»åŠ¡ {task_id} å·²å–æ¶ˆ"
            }
        else:
            raise HTTPException(status_code=400, detail="æ— æ³•å–æ¶ˆè¯¥ä»»åŠ¡ï¼ˆå¯èƒ½å·²å®Œæˆæˆ–ä¸å­˜åœ¨ï¼‰")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/okx/statistics")
async def get_okx_download_statistics(
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–OKXä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # ä»æ•°æ®åº“è·å–çœŸå®çš„ç»Ÿè®¡æ•°æ®
        query = text("""
        SELECT 
            COUNT(*) as total_downloads,
            SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as successful_downloads,
            SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_downloads,
            SUM(success_count) as total_files_processed,
            SUM(total_records) as total_records_downloaded
        FROM data_collection_tasks 
        WHERE exchange = 'okx'
        """)
        
        result = await db.execute(query)
        row = result.fetchone()
        
        download_statistics = {
            "total_downloads": row[0] or 0,
            "successful_downloads": row[1] or 0,
            "failed_downloads": row[2] or 0,
            "total_files_processed": row[3] or 0,
            "total_records_downloaded": row[4] or 0
        }
        
        return {
            "success": True,
            "data": {
                "download_statistics": download_statistics,
                "supported_symbols": {
                    "tick_data": okx_data_downloader.supported_tick_symbols,
                    "kline_data": okx_data_downloader.supported_kline_symbols
                },
                "supported_timeframes": okx_data_downloader.supported_timeframes,
                "data_directories": {
                    "tick_data": str(okx_data_downloader.okx_tick_dir),
                    "kline_data": str(okx_data_downloader.okx_kline_dir)
                }
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä¸‹è½½ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/okx/cleanup")
async def cleanup_okx_completed_tasks(
    days_old: int = 7,
    current_user = Depends(get_current_active_user)
):
    """æ¸…ç†å®Œæˆçš„OKXä¸‹è½½ä»»åŠ¡"""
    try:
        await okx_data_downloader.clean_completed_tasks(days_old)
        
        return {
            "success": True,
            "message": f"å·²æ¸…ç† {days_old} å¤©å‰çš„å®Œæˆä»»åŠ¡"
        }
        
    except Exception as e:
        logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ================================
# å¢å¼ºç‰ˆæ•°æ®è´¨é‡æ£€æŸ¥API
# ================================

@router.post("/okx/quality/check")
async def check_data_quality(
    request: Dict[str, Any],
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """æ•°æ®è´¨é‡æ£€æŸ¥ - å¢å¼ºç‰ˆ"""
    try:
        symbol = request.get('symbol')
        timeframe = request.get('timeframe')
        start_date = request.get('start_date')  # æ ¼å¼: 20240101
        end_date = request.get('end_date')      # æ ¼å¼: 20240131
        
        if not all([symbol, timeframe, start_date, end_date]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°: symbol, timeframe, start_date, end_date")
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        
        # ä½¿ç”¨å¢å¼ºç‰ˆè´¨é‡æ£€æŸ¥å™¨
        from app.services.okx_data_downloader_enhanced import DataQualityChecker
        
        quality_score, quality_issues = await DataQualityChecker.check_kline_data_quality(
            db, symbol, timeframe, start_dt, end_dt
        )
        
        # æŸ¥è¯¢æ•°æ®åŸºæœ¬ç»Ÿè®¡
        result = await db.execute(
            select(
                func.count(MarketData.id).label('total_records'),
                func.min(MarketData.timestamp).label('earliest_date'),
                func.max(MarketData.timestamp).label('latest_date'),
                func.avg(MarketData.volume).label('avg_volume')
            ).where(
                and_(
                    MarketData.exchange == "okx",
                    MarketData.symbol == symbol,
                    MarketData.timeframe == timeframe,
                    MarketData.timestamp >= start_dt,
                    MarketData.timestamp <= end_dt
                )
            )
        )
        
        stats = result.first()
        
        return {
            "success": True,
            "data": {
                "symbol": symbol,
                "timeframe": timeframe,
                "date_range": f"{start_date} - {end_date}",
                "quality_score": round(quality_score, 1),
                "quality_grade": _get_quality_grade(quality_score),
                "quality_issues": quality_issues,
                "statistics": {
                    "total_records": stats.total_records or 0,
                    "earliest_date": stats.earliest_date.isoformat() if stats.earliest_date else None,
                    "latest_date": stats.latest_date.isoformat() if stats.latest_date else None,
                    "average_volume": float(stats.avg_volume) if stats.avg_volume else 0
                },
                "recommendations": _get_quality_recommendations(quality_score, quality_issues),
                "checked_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/okx/repair/missing")
async def repair_missing_data(
    request: Dict[str, Any],
    background_tasks: BackgroundTasks,
    current_user = Depends(get_current_active_user)
):
    """ä¿®å¤ç¼ºå¤±æ•°æ® - æ™ºèƒ½è¡¥å…¨"""
    try:
        symbol = request.get('symbol')
        timeframe = request.get('timeframe')
        start_date = request.get('start_date')
        end_date = request.get('end_date')
        
        if not all([symbol, timeframe, start_date, end_date]):
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°")
        
        # ä½¿ç”¨å¢å¼ºç‰ˆä¸‹è½½å™¨è¿›è¡Œä¿®å¤
        repair_task = await enhanced_okx_downloader.create_enhanced_kline_task(
            symbols=[symbol],
            timeframes=[timeframe],
            start_date=start_date,
            end_date=end_date
        )
        
        # åå°æ‰§è¡Œä¿®å¤ä»»åŠ¡
        background_tasks.add_task(
            enhanced_okx_downloader.execute_enhanced_kline_task,
            repair_task.task_id
        )
        
        return {
            "success": True,
            "data": {
                "repair_task_id": repair_task.task_id,
                "symbol": symbol,
                "timeframe": timeframe,
                "date_range": f"{start_date} - {end_date}",
                "message": "æ•°æ®ä¿®å¤ä»»åŠ¡å·²å¯åŠ¨",
                "estimated_time": "5-15åˆ†é’Ÿ"
            }
        }
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æ•°æ®ä¿®å¤å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _normalize_symbol_for_query(symbol: str) -> str:
    """
    å°†å‰ç«¯æŸ¥è¯¢çš„ç¬¦å·æ ¼å¼è½¬æ¢ä¸ºæ•°æ®åº“ä¸­å­˜å‚¨çš„æ ¼å¼
    å‰ç«¯æ ¼å¼ç¤ºä¾‹:
    - Tickæ•°æ®: "BTC" -> æ•°æ®åº“æ ¼å¼: "BTC/USDT"
    - Tickæ•°æ®: "BTC/USDT" -> æ•°æ®åº“æ ¼å¼: "BTC/USDT" (ä¸å˜)
    - Kçº¿æ•°æ®: "BTC-USDT-SWAP" -> æ•°æ®åº“æ ¼å¼: "BTCUSDTUSDT"
    """
    if not symbol:
        return symbol
    
    # å¦‚æœæ˜¯æœŸè´§åˆçº¦æ ¼å¼ BTC-USDT-SWAP
    if '-SWAP' in symbol:
        # BTC-USDT-SWAP -> BTC/USDT:USDT -> BTCUSDTUSDT
        base_symbol = symbol.replace('-SWAP', '').replace('-', '/')
        ccxt_format = base_symbol + ':USDT'  # BTC/USDT:USDT
        normalized = ccxt_format.replace('/', '').replace(':', '')  # BTCUSDTUSDT
    elif '-' in symbol:
        # åŒ…å«çŸ­æ¨ªçº¿çš„æ ¼å¼: BTC-USDT -> BTCUSDT
        normalized = symbol.replace('-', '').replace('/', '').replace(':', '')
    elif '/' in symbol and 'USDT' in symbol:
        # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼: BTC/USDT -> BTC/USDT (ä¸å˜)
        normalized = symbol
    else:
        # ç®€å•æ ¼å¼ (ä¸»è¦æ˜¯tickæ•°æ®): BTC -> BTC/USDT
        # è¿™æ˜¯é’ˆå¯¹tickæ•°æ®æŸ¥è¯¢çš„ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºtickæ•°æ®å­˜å‚¨æ ¼å¼æ˜¯ BTC/USDT
        normalized = f"{symbol}/USDT"
    
    logger.info(f"ğŸ”„ ç¬¦å·è½¬æ¢: {symbol} -> {normalized}")
    print(f"DEBUG: ç¬¦å·è½¬æ¢: {symbol} -> {normalized}")
    return normalized

def _calculate_summary_statistics(data_info: List[Dict], data_type: str, symbol: str = None) -> Dict[str, Any]:
    """
    è®¡ç®—æ•°æ®æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
    åŒ…æ‹¬ï¼šæ€»è®°å½•æ•°ã€æ•°æ®å®Œæ•´åº¦ã€æ—¶é—´èŒƒå›´ã€å¯ç”¨æ—¶é—´æ¡†æ¶ç­‰
    """
    if not data_info:
        return {
            "total_records": 0,
            "data_completeness_percent": 0,
            "timeframes_available": 0,
            "earliest_date": None,
            "latest_date": None,
            "days_span": 0
        }
    
    summary = {
        "total_records": 0,
        "data_completeness_percent": 0,
        "timeframes_available": 0,
        "earliest_date": None,
        "latest_date": None,
        "days_span": 0
    }
    
    if data_type == 'kline':
        if symbol:
            # ç‰¹å®šäº¤æ˜“å¯¹çš„è¯¦ç»†ç»Ÿè®¡
            total_records = sum(row.get('record_count', 0) for row in data_info)
            timeframes = len(data_info)
            
            # è·å–æ—¶é—´èŒƒå›´
            all_start_dates = [row.get('start_date') for row in data_info if row.get('start_date')]
            all_end_dates = [row.get('end_date') for row in data_info if row.get('end_date')]
            
            if all_start_dates and all_end_dates:
                earliest_date = min(all_start_dates)
                latest_date = max(all_end_dates)
                
                # è®¡ç®—å¤©æ•°èŒƒå›´ï¼ˆåŸºäºå­—ç¬¦ä¸²æ—¥æœŸï¼‰
                try:
                    if isinstance(earliest_date, str) and isinstance(latest_date, str):
                        earliest_dt = datetime.fromisoformat(earliest_date.replace('Z', '+00:00'))
                        latest_dt = datetime.fromisoformat(latest_date.replace('Z', '+00:00'))
                        days_span = (latest_dt - earliest_dt).days + 1
                    else:
                        days_span = 0
                except:
                    days_span = 0
                
                # åŸºäº1åˆ†é’Ÿæ•°æ®ä¼°ç®—å®Œæ•´åº¦
                expected_records_per_day = 1440  # 1440åˆ†é’Ÿ/å¤©
                expected_total = days_span * expected_records_per_day
                
                # æŸ¥æ‰¾1åˆ†é’Ÿæ—¶é—´æ¡†æ¶çš„è®°å½•æ•°
                one_min_records = 0
                for row in data_info:
                    if row.get('timeframe') == '1m':
                        one_min_records = row.get('record_count', 0)
                        break
                
                if expected_total > 0 and one_min_records > 0:
                    completeness = min(100, (one_min_records / expected_total) * 100)
                else:
                    completeness = 100  # å‡è®¾å…¶ä»–æ—¶é—´æ¡†æ¶æ˜¯å®Œæ•´çš„
                
                summary.update({
                    "total_records": total_records,
                    "data_completeness_percent": round(completeness, 2),
                    "timeframes_available": timeframes,
                    "earliest_date": earliest_date,
                    "latest_date": latest_date,
                    "days_span": days_span
                })
        else:
            # æ‰€æœ‰äº¤æ˜“å¯¹çš„æ¦‚è§ˆç»Ÿè®¡
            total_records = sum(row.get('total_records', 0) for row in data_info)
            total_symbols = len(data_info)
            
            # è·å–å…¨éƒ¨æ—¶é—´èŒƒå›´
            all_earliest = [row.get('earliest_date') for row in data_info if row.get('earliest_date')]
            all_latest = [row.get('latest_date') for row in data_info if row.get('latest_date')]
            
            if all_earliest and all_latest:
                earliest_date = min(all_earliest)
                latest_date = max(all_latest)
            else:
                earliest_date = latest_date = None
            
            summary.update({
                "total_records": total_records,
                "total_symbols": total_symbols,
                "data_completeness_percent": 95,  # é¢„ä¼°å€¼
                "earliest_date": earliest_date,
                "latest_date": latest_date
            })
    
    elif data_type == 'tick':
        # Tickæ•°æ®ç»Ÿè®¡
        total_records = sum(row.get('record_count', 0) for row in data_info)
        
        summary.update({
            "total_records": total_records,
            "data_completeness_percent": 100,  # Tickæ•°æ®é€šå¸¸æ˜¯å®Œæ•´çš„
            "data_type": "tick"
        })
    
    logger.info(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦: {summary}")
    return summary

# æ•°æ®æŸ¥è¯¢API
@router.get("/query")
async def query_data_availability(
    data_type: str,  # 'kline' or 'tick'
    exchange: str = 'okx',
    symbol: str = None,
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """æŸ¥è¯¢æ•°æ®åº“ä¸­çš„çœŸå®æ•°æ®æƒ…å†µ"""
    try:
        # ğŸ”„ è½¬æ¢ç¬¦å·æ ¼å¼ä¸ºæ•°æ®åº“å­˜å‚¨æ ¼å¼
        normalized_symbol = _normalize_symbol_for_query(symbol) if symbol else None
        logger.info(f"ğŸ” æŸ¥è¯¢æ•°æ®: data_type={data_type}, original_symbol={symbol}, normalized_symbol={normalized_symbol}")
        print(f"DEBUG: æŸ¥è¯¢æ•°æ® - data_type={data_type}, original_symbol={symbol}, normalized_symbol={normalized_symbol}")
        
        if data_type == 'kline':
            # æŸ¥è¯¢Kçº¿æ•°æ®
            if symbol:
                # æŸ¥è¯¢ç‰¹å®šäº¤æ˜“å¯¹çš„æ•°æ®
                query = text("""
                SELECT 
                    symbol,
                    timeframe,
                    MIN(timestamp) as start_date,
                    MAX(timestamp) as end_date,
                    COUNT(*) as record_count
                FROM market_data 
                WHERE exchange = :exchange AND symbol = :symbol
                GROUP BY symbol, timeframe
                ORDER BY symbol, timeframe
                """)
                result = await db.execute(query, {"exchange": exchange, "symbol": normalized_symbol})
            else:
                # æŸ¥è¯¢æ‰€æœ‰äº¤æ˜“å¯¹çš„æ•°æ®æ¦‚è§ˆ
                query = text("""
                SELECT 
                    symbol,
                    COUNT(DISTINCT timeframe) as timeframes_count,
                    MIN(timestamp) as earliest_date,
                    MAX(timestamp) as latest_date,
                    COUNT(*) as total_records
                FROM market_data 
                WHERE exchange = :exchange
                GROUP BY symbol
                ORDER BY symbol
                """)
                result = await db.execute(query, {"exchange": exchange})
                
        elif data_type == 'tick':
            # æŸ¥è¯¢Tickæ•°æ®
            if symbol:
                query = text("""
                SELECT 
                    symbol,
                    MIN(timestamp) as start_timestamp,
                    MAX(timestamp) as end_timestamp,
                    COUNT(*) as record_count,
                    MIN(created_at) as first_created,
                    MAX(created_at) as last_created
                FROM tick_data 
                WHERE exchange = :exchange AND symbol = :symbol
                GROUP BY symbol
                """)
                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                query_params = {"exchange": exchange, "symbol": normalized_symbol}
                logger.info(f"ğŸ” æ‰§è¡ŒTickæ•°æ®æŸ¥è¯¢: exchange={exchange}, symbol={normalized_symbol}")
                logger.info(f"ğŸ” æŸ¥è¯¢SQL: {query.text}")
                logger.info(f"ğŸ” æŸ¥è¯¢å‚æ•°: {query_params}")
                result = await db.execute(query, query_params)
                logger.info(f"ğŸ” æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå‡†å¤‡è·å–ç»“æœ...")
            else:
                query = text("""
                SELECT 
                    symbol,
                    MIN(timestamp) as earliest_timestamp,
                    MAX(timestamp) as latest_timestamp,
                    COUNT(*) as total_records
                FROM tick_data 
                WHERE exchange = :exchange
                GROUP BY symbol
                ORDER BY symbol
                """)
                result = await db.execute(query, {"exchange": exchange})
        else:
            raise HTTPException(status_code=400, detail="data_typeå¿…é¡»æ˜¯'kline'æˆ–'tick'")
        
        rows = result.fetchall()
        logger.info(f"ğŸ” æŸ¥è¯¢ç»“æœrowsæ•°é‡: {len(rows)}")
        print(f"DEBUG: æŸ¥è¯¢ç»“æœrowsæ•°é‡: {len(rows)}")
        if rows:
            logger.info(f"ğŸ” ç¬¬ä¸€è¡Œæ•°æ®: {dict(rows[0]._mapping)}")
            print(f"DEBUG: ç¬¬ä¸€è¡Œæ•°æ®: {dict(rows[0]._mapping)}")
        else:
            logger.info(f"ğŸ” æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®è¡Œ")
            print(f"DEBUG: æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®è¡Œ")
        
        # æ ¼å¼åŒ–è¿”å›æ•°æ®
        data_info = []
        for row in rows:
            row_dict = dict(row._mapping)
            
            # è½¬æ¢æ—¶é—´æˆ³æ ¼å¼
            if data_type == 'kline':
                if row_dict.get('start_date'):
                    row_dict['start_date'] = row_dict['start_date'].isoformat() if hasattr(row_dict['start_date'], 'isoformat') else str(row_dict['start_date'])
                if row_dict.get('end_date'):
                    row_dict['end_date'] = row_dict['end_date'].isoformat() if hasattr(row_dict['end_date'], 'isoformat') else str(row_dict['end_date'])
                if row_dict.get('earliest_date'):
                    row_dict['earliest_date'] = row_dict['earliest_date'].isoformat() if hasattr(row_dict['earliest_date'], 'isoformat') else str(row_dict['earliest_date'])
                if row_dict.get('latest_date'):
                    row_dict['latest_date'] = row_dict['latest_date'].isoformat() if hasattr(row_dict['latest_date'], 'isoformat') else str(row_dict['latest_date'])
            
            elif data_type == 'tick':
                # Tickæ•°æ®æ—¶é—´æˆ³æ˜¯BIGINTï¼Œéœ€è¦è½¬æ¢
                if row_dict.get('start_timestamp'):
                    row_dict['start_date'] = datetime.fromtimestamp(row_dict['start_timestamp']/1000).isoformat() if row_dict['start_timestamp'] else None
                if row_dict.get('end_timestamp'):
                    row_dict['end_date'] = datetime.fromtimestamp(row_dict['end_timestamp']/1000).isoformat() if row_dict['end_timestamp'] else None
                if row_dict.get('earliest_timestamp'):
                    row_dict['earliest_date'] = datetime.fromtimestamp(row_dict['earliest_timestamp']/1000).isoformat() if row_dict['earliest_timestamp'] else None
                if row_dict.get('latest_timestamp'):
                    row_dict['latest_date'] = datetime.fromtimestamp(row_dict['latest_timestamp']/1000).isoformat() if row_dict['latest_timestamp'] else None
            
            data_info.append(row_dict)
        
        # ğŸ”¢ è®¡ç®—æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        summary_stats = _calculate_summary_statistics(data_info, data_type, symbol)
        
        return {
            "success": True,
            "data": {
                "data_type": data_type,
                "exchange": exchange,
                "symbol": symbol,
                "query_result": data_info,
                "total_symbols": len(data_info) if not symbol else 1,
                "summary": summary_stats
            }
        }
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/timeframes")
async def get_available_timeframes(
    symbol: str,
    exchange: str = 'okx',
    current_user = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æŒ‡å®šäº¤æ˜“å¯¹çš„å¯ç”¨æ—¶é—´æ¡†æ¶"""
    try:
        query = text("""
        SELECT 
            timeframe,
            MIN(timestamp) as start_date,
            MAX(timestamp) as end_date,
            COUNT(*) as record_count
        FROM market_data 
        WHERE exchange = :exchange AND symbol = :symbol
        GROUP BY timeframe
        ORDER BY 
            CASE timeframe 
                WHEN '1m' THEN 1 WHEN '5m' THEN 2 WHEN '15m' THEN 3 
                WHEN '30m' THEN 4 WHEN '1h' THEN 5 WHEN '2h' THEN 6 
                WHEN '4h' THEN 7 WHEN '1d' THEN 8 WHEN '1w' THEN 9 
                ELSE 99 END
        """)
        
        result = await db.execute(query, {"exchange": exchange, "symbol": symbol})
        rows = result.fetchall()
        
        timeframes_data = []
        for row in rows:
            timeframes_data.append({
                "timeframe": row.timeframe,
                "start_date": row.start_date.isoformat() if row.start_date else None,
                "end_date": row.end_date.isoformat() if row.end_date else None,
                "record_count": row.record_count
            })
        
        return {
            "success": True,
            "data": {
                "symbol": symbol,
                "exchange": exchange,
                "timeframes": timeframes_data
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ—¶é—´æ¡†æ¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def _estimate_completion_time(task) -> Optional[str]:
    """ä¼°ç®—ä»»åŠ¡å®Œæˆæ—¶é—´"""
    try:
        if task.status != TaskStatus.RUNNING or task.progress <= 0:
            return None
        
        elapsed_time = (datetime.now() - task.started_at).total_seconds()
        remaining_progress = 100 - task.progress
        estimated_remaining_seconds = (elapsed_time / task.progress) * remaining_progress
        
        estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_seconds)
        return estimated_completion.isoformat()
        
    except Exception:
        return None

def _get_quality_grade(quality_score: float) -> str:
    """æ ¹æ®è´¨é‡è¯„åˆ†è·å–ç­‰çº§"""
    if quality_score >= 95:
        return "ä¼˜ç§€"
    elif quality_score >= 85:
        return "è‰¯å¥½"
    elif quality_score >= 70:
        return "ä¸€èˆ¬"
    elif quality_score >= 50:
        return "è¾ƒå·®"
    else:
        return "å¾ˆå·®"

def _get_quality_recommendations(quality_score: float, quality_issues: List[str]) -> List[str]:
    """æ ¹æ®è´¨é‡è¯„åˆ†å’Œé—®é¢˜ç”Ÿæˆå»ºè®®"""
    recommendations = []
    
    if quality_score < 95:
        recommendations.append("å»ºè®®ä½¿ç”¨å¢å¼ºç‰ˆä¸‹è½½å™¨é‡æ–°ä¸‹è½½æ•°æ®")
    
    if "æ•°æ®ä¸å®Œæ•´" in str(quality_issues):
        recommendations.append("ä½¿ç”¨æ•°æ®ä¿®å¤åŠŸèƒ½è¡¥å…¨ç¼ºå¤±çš„æ•°æ®")
    
    if "æ—¶é—´é—´éš™" in str(quality_issues):
        recommendations.append("æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™æµè®¾ç½®")
    
    if "ä»·æ ¼å¼‚å¸¸" in str(quality_issues):
        recommendations.append("éªŒè¯æ•°æ®æºå’Œæ ¼å¼è½¬æ¢é€»è¾‘")
    
    if not recommendations:
        recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†")
    
    return recommendations