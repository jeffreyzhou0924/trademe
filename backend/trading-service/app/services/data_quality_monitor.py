"""
æ•°æ®è´¨é‡ç›‘æ§æœåŠ¡
ç›‘æ§Kçº¿å’ŒTickæ•°æ®çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from sqlalchemy.ext.asyncio import AsyncSession
import logging
import pandas as pd
import numpy as np
from decimal import Decimal

from app.database import AsyncSessionLocal

logger = logging.getLogger(__name__)

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.quality_thresholds = {
            'completeness_min': 95.0,      # æœ€ä½å®Œæ•´æ€§è¦æ±‚95%
            'price_volatility_max': 50.0,  # ä»·æ ¼æ³¢åŠ¨ä¸Šé™50%
            'volume_anomaly_factor': 10.0, # æˆäº¤é‡å¼‚å¸¸å› å­
            'time_gap_max_minutes': 5      # æœ€å¤§æ—¶é—´ç¼ºå£5åˆ†é’Ÿ
        }
    
    async def run_comprehensive_quality_check(
        self,
        exchange: str,
        symbol: str,
        timeframe: str,
        check_days: int = 7,
        db: Optional[AsyncSession] = None
    ) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
        
        should_close_db = False
        if not db:
            db = AsyncSessionLocal()
            should_close_db = True
        
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=check_days)
            
            logger.info(f"å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥: {exchange} {symbol} {timeframe} ({check_days}å¤©)")
            
            # 1. å®Œæ•´æ€§æ£€æŸ¥
            completeness_result = await self._check_data_completeness(
                db, exchange, symbol, timeframe, start_date, end_date
            )
            
            # 2. ä»·æ ¼åˆç†æ€§æ£€æŸ¥
            price_result = await self._check_price_validity(
                db, exchange, symbol, timeframe, start_date, end_date
            )
            
            # 3. æ—¶é—´è¿ç»­æ€§æ£€æŸ¥
            continuity_result = await self._check_time_continuity(
                db, exchange, symbol, timeframe, start_date, end_date
            )
            
            # 4. æˆäº¤é‡å¼‚å¸¸æ£€æŸ¥
            volume_result = await self._check_volume_anomalies(
                db, exchange, symbol, timeframe, start_date, end_date
            )
            
            # 5. æ•°æ®é‡å¤æ£€æŸ¥
            duplication_result = await self._check_data_duplication(
                db, exchange, symbol, timeframe, start_date, end_date
            )
            
            # è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†
            quality_score = self._calculate_quality_score(
                completeness_result, price_result, continuity_result, 
                volume_result, duplication_result
            )
            
            # ä¿å­˜è´¨é‡æ£€æŸ¥ç»“æœ
            await self._save_quality_metrics(
                db, exchange, symbol, timeframe, start_date, end_date,
                completeness_result, price_result, continuity_result,
                volume_result, duplication_result, quality_score
            )
            
            result = {
                'symbol': symbol,
                'timeframe': timeframe,
                'check_period': f"{start_date} - {end_date}",
                'quality_score': quality_score,
                'completeness': completeness_result,
                'price_validity': price_result,
                'time_continuity': continuity_result,
                'volume_analysis': volume_result,
                'duplication_check': duplication_result,
                'recommendation': self._generate_recommendations(quality_score, completeness_result, price_result)
            }
            
            logger.info(f"æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ: è¯„åˆ† {quality_score:.1f}/100")
            return result
            
        finally:
            if should_close_db:
                await db.close()
    
    async def _check_data_completeness(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        
        # è®¡ç®—æœŸæœ›çš„Kçº¿æ•°é‡
        timeframe_minutes = {
            '1m': 1, '5m': 5, '15m': 15, '1h': 60, '4h': 240, '1d': 1440
        }.get(timeframe, 60)
        
        total_minutes = int((end_date - start_date).total_seconds() / 60)
        expected_count = total_minutes // timeframe_minutes
        
        # æŸ¥è¯¢å®é™…æ•°æ®æ•°é‡
        query = """
        SELECT COUNT(*) as actual_count,
               MIN(open_time) as first_timestamp,
               MAX(open_time) as last_timestamp
        FROM kline_data 
        WHERE exchange = :exchange 
          AND symbol = :symbol 
          AND timeframe = :timeframe
          AND open_time >= :start_ts 
          AND open_time <= :end_ts
        """
        
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        result = await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_ts': start_ts,
            'end_ts': end_ts
        })
        
        row = result.fetchone()
        actual_count = row[0] if row else 0
        first_ts = row[1] if row else None
        last_ts = row[2] if row else None
        
        # è®¡ç®—å®Œæ•´æ€§
        completeness_percent = (actual_count / expected_count * 100) if expected_count > 0 else 0
        missing_count = max(0, expected_count - actual_count)
        
        return {
            'expected_count': expected_count,
            'actual_count': actual_count,
            'missing_count': missing_count,
            'completeness_percent': completeness_percent,
            'first_timestamp': datetime.fromtimestamp(first_ts / 1000) if first_ts else None,
            'last_timestamp': datetime.fromtimestamp(last_ts / 1000) if last_ts else None,
            'is_complete': completeness_percent >= self.quality_thresholds['completeness_min']
        }
    
    async def _check_price_validity(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§"""
        
        query = """
        SELECT open_price, high_price, low_price, close_price, volume, open_time
        FROM kline_data 
        WHERE exchange = :exchange 
          AND symbol = :symbol 
          AND timeframe = :timeframe
          AND open_time >= :start_ts 
          AND open_time <= :end_ts
        ORDER BY open_time ASC
        """
        
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        result = await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_ts': start_ts,
            'end_ts': end_ts
        })
        
        rows = result.fetchall()
        
        if not rows:
            return {'valid': False, 'error': 'æ— æ•°æ®'}
        
        # è½¬æ¢ä¸ºæ•°å€¼è¿›è¡Œåˆ†æ
        prices = []
        volumes = []
        invalid_records = []
        
        for i, row in enumerate(rows):
            open_p, high_p, low_p, close_p, volume, timestamp = row
            
            # æ£€æŸ¥OHLCé€»è¾‘å…³ç³»
            if not (low_p <= open_p <= high_p and low_p <= close_p <= high_p):
                invalid_records.append({
                    'timestamp': timestamp,
                    'issue': 'OHLCé€»è¾‘é”™è¯¯',
                    'data': f"O:{open_p} H:{high_p} L:{low_p} C:{close_p}"
                })
            
            # æ£€æŸ¥é›¶å€¼æˆ–è´Ÿå€¼
            if any(p <= 0 for p in [open_p, high_p, low_p, close_p]):
                invalid_records.append({
                    'timestamp': timestamp,
                    'issue': 'ä»·æ ¼é›¶å€¼æˆ–è´Ÿå€¼',
                    'data': f"O:{open_p} H:{high_p} L:{low_p} C:{close_p}"
                })
            
            # æ£€æŸ¥å¼‚å¸¸æ³¢åŠ¨
            if i > 0:
                prev_close = float(rows[i-1][3])
                current_open = float(open_p)
                volatility = abs(current_open - prev_close) / prev_close * 100
                
                if volatility > self.quality_thresholds['price_volatility_max']:
                    invalid_records.append({
                        'timestamp': timestamp,
                        'issue': f'å¼‚å¸¸æ³¢åŠ¨ {volatility:.2f}%',
                        'data': f"å‰æ”¶:{prev_close} å½“å¼€:{current_open}"
                    })
            
            prices.append(float(close_p))
            volumes.append(float(volume))
        
        # ç»Ÿè®¡åˆ†æ
        price_stats = {
            'mean': np.mean(prices),
            'std': np.std(prices),
            'min': np.min(prices),
            'max': np.max(prices),
            'median': np.median(prices)
        }
        
        volume_stats = {
            'mean': np.mean(volumes),
            'std': np.std(volumes),
            'min': np.min(volumes),
            'max': np.max(volumes)
        }
        
        return {
            'valid': len(invalid_records) == 0,
            'total_records': len(rows),
            'invalid_count': len(invalid_records),
            'invalid_records': invalid_records[:10],  # åªè¿”å›å‰10ä¸ªé”™è¯¯
            'price_statistics': price_stats,
            'volume_statistics': volume_stats,
            'validity_percent': (len(rows) - len(invalid_records)) / len(rows) * 100
        }
    
    async def _check_time_continuity(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ—¶é—´è¿ç»­æ€§"""
        
        query = """
        SELECT open_time 
        FROM kline_data 
        WHERE exchange = :exchange 
          AND symbol = :symbol 
          AND timeframe = :timeframe
          AND open_time >= :start_ts 
          AND open_time <= :end_ts
        ORDER BY open_time ASC
        """
        
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        result = await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_ts': start_ts,
            'end_ts': end_ts
        })
        
        timestamps = [row[0] for row in result.fetchall()]
        
        if not timestamps:
            return {'continuous': False, 'gaps': [], 'gap_count': 0}
        
        # è®¡ç®—æ—¶é—´é—´éš”
        timeframe_ms = {
            '1m': 60 * 1000, '5m': 5 * 60 * 1000, '15m': 15 * 60 * 1000,
            '1h': 60 * 60 * 1000, '4h': 4 * 60 * 60 * 1000, '1d': 24 * 60 * 60 * 1000
        }.get(timeframe, 60 * 60 * 1000)
        
        # æ£€æŸ¥æ—¶é—´ç¼ºå£
        gaps = []
        for i in range(1, len(timestamps)):
            expected_time = timestamps[i-1] + timeframe_ms
            actual_time = timestamps[i]
            
            if actual_time > expected_time + (timeframe_ms * 0.1):  # å…è®¸10%è¯¯å·®
                gap_duration_minutes = (actual_time - expected_time) / (60 * 1000)
                
                if gap_duration_minutes > self.quality_thresholds['time_gap_max_minutes']:
                    gaps.append({
                        'start_time': datetime.fromtimestamp(timestamps[i-1] / 1000),
                        'end_time': datetime.fromtimestamp(actual_time / 1000),
                        'gap_duration_minutes': gap_duration_minutes
                    })
        
        return {
            'continuous': len(gaps) == 0,
            'total_records': len(timestamps),
            'gap_count': len(gaps),
            'gaps': gaps[:20],  # æœ€å¤šè¿”å›20ä¸ªç¼ºå£
            'continuity_percent': (len(timestamps) - len(gaps)) / len(timestamps) * 100 if timestamps else 0
        }
    
    async def _check_volume_anomalies(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸"""
        
        query = """
        SELECT volume, open_time 
        FROM kline_data 
        WHERE exchange = :exchange 
          AND symbol = :symbol 
          AND timeframe = :timeframe
          AND open_time >= :start_ts 
          AND open_time <= :end_ts
          AND volume IS NOT NULL
        ORDER BY open_time ASC
        """
        
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        result = await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_ts': start_ts,
            'end_ts': end_ts
        })
        
        rows = result.fetchall()
        
        if not rows:
            return {'anomalies_detected': False, 'anomaly_count': 0}
        
        volumes = [float(row[0]) for row in rows]
        timestamps = [row[1] for row in rows]
        
        # è®¡ç®—æˆäº¤é‡ç»Ÿè®¡
        volume_mean = np.mean(volumes)
        volume_std = np.std(volumes)
        
        # æ£€æµ‹å¼‚å¸¸å€¼ (è¶…è¿‡å¹³å‡å€¼+Nå€æ ‡å‡†å·®)
        anomaly_threshold = volume_mean + (self.quality_thresholds['volume_anomaly_factor'] * volume_std)
        
        anomalies = []
        for i, (volume, timestamp) in enumerate(zip(volumes, timestamps)):
            if volume > anomaly_threshold:
                anomalies.append({
                    'timestamp': datetime.fromtimestamp(timestamp / 1000),
                    'volume': volume,
                    'anomaly_factor': volume / volume_mean,
                    'description': f'æˆäº¤é‡å¼‚å¸¸: {volume:.2f} (å¹³å‡å€¼çš„{volume/volume_mean:.1f}å€)'
                })
        
        return {
            'anomalies_detected': len(anomalies) > 0,
            'anomaly_count': len(anomalies),
            'anomalies': anomalies[:10],  # æœ€å¤šè¿”å›10ä¸ªå¼‚å¸¸
            'volume_statistics': {
                'mean': volume_mean,
                'std': volume_std,
                'min': np.min(volumes),
                'max': np.max(volumes),
                'median': np.median(volumes)
            },
            'anomaly_threshold': anomaly_threshold
        }
    
    async def _check_data_duplication(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®é‡å¤"""
        
        query = """
        SELECT open_time, COUNT(*) as count
        FROM kline_data 
        WHERE exchange = :exchange 
          AND symbol = :symbol 
          AND timeframe = :timeframe
          AND open_time >= :start_ts 
          AND open_time <= :end_ts
        GROUP BY open_time
        HAVING COUNT(*) > 1
        ORDER BY count DESC
        """
        
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        
        result = await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_ts': start_ts,
            'end_ts': end_ts
        })
        
        duplicates = result.fetchall()
        
        total_duplicates = sum(row[1] - 1 for row in duplicates)  # å‡å»æ­£å¸¸è®°å½•
        
        return {
            'has_duplicates': len(duplicates) > 0,
            'duplicate_timestamps': len(duplicates),
            'total_duplicate_records': total_duplicates,
            'duplicate_details': [
                {
                    'timestamp': datetime.fromtimestamp(row[0] / 1000),
                    'duplicate_count': row[1]
                }
                for row in duplicates[:10]
            ]
        }
    
    def _calculate_quality_score(
        self,
        completeness: Dict[str, Any],
        price_validity: Dict[str, Any],
        continuity: Dict[str, Any],
        volume_analysis: Dict[str, Any],
        duplication: Dict[str, Any]
    ) -> float:
        """è®¡ç®—ç»¼åˆæ•°æ®è´¨é‡è¯„åˆ†"""
        
        # æƒé‡åˆ†é…
        weights = {
            'completeness': 0.3,    # å®Œæ•´æ€§æƒé‡30%
            'price_validity': 0.25, # ä»·æ ¼æœ‰æ•ˆæ€§æƒé‡25%
            'continuity': 0.25,     # è¿ç»­æ€§æƒé‡25%
            'volume': 0.1,          # æˆäº¤é‡æƒé‡10%
            'duplication': 0.1      # é‡å¤æ£€æŸ¥æƒé‡10%
        }
        
        # è®¡ç®—å„é¡¹å¾—åˆ†
        completeness_score = min(completeness.get('completeness_percent', 0), 100)
        
        price_score = price_validity.get('validity_percent', 0) if price_validity.get('valid') else 0
        
        continuity_score = continuity.get('continuity_percent', 0) if continuity.get('continuous') else 0
        
        volume_score = 100 if not volume_analysis.get('anomalies_detected') else max(0, 100 - volume_analysis.get('anomaly_count', 0) * 5)
        
        duplication_score = 100 if not duplication.get('has_duplicates') else max(0, 100 - duplication.get('duplicate_timestamps', 0) * 2)
        
        # åŠ æƒå¹³å‡
        total_score = (
            completeness_score * weights['completeness'] +
            price_score * weights['price_validity'] +
            continuity_score * weights['continuity'] +
            volume_score * weights['volume'] +
            duplication_score * weights['duplication']
        )
        
        return round(total_score, 2)
    
    def _generate_recommendations(
        self,
        quality_score: float,
        completeness: Dict[str, Any],
        price_validity: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡æ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        if quality_score < 70:
            recommendations.append("ğŸ”´ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°ä¸‹è½½å†å²æ•°æ®")
        elif quality_score < 85:
            recommendations.append("ğŸŸ¡ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…ç†")
        else:
            recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½")
        
        if completeness.get('completeness_percent', 0) < 95:
            missing = completeness.get('missing_count', 0)
            recommendations.append(f"ğŸ“‰ æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå¤± {missing} æ¡è®°å½•ï¼Œå»ºè®®è¡¥å…¨")
        
        if not price_validity.get('valid', True):
            invalid_count = price_validity.get('invalid_count', 0)
            recommendations.append(f"ğŸ’° å‘ç° {invalid_count} æ¡ä»·æ ¼å¼‚å¸¸è®°å½•ï¼Œå»ºè®®äººå·¥å®¡æ ¸")
        
        return recommendations
    
    async def _save_quality_metrics(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime,
        completeness: Dict[str, Any],
        price_validity: Dict[str, Any],
        continuity: Dict[str, Any],
        volume_analysis: Dict[str, Any],
        duplication: Dict[str, Any],
        quality_score: float
    ):
        """ä¿å­˜è´¨é‡æ£€æŸ¥ç»“æœåˆ°æ•°æ®åº“"""
        
        query = """
        INSERT OR REPLACE INTO data_quality_metrics 
        (exchange, symbol, timeframe, date_range_start, date_range_end,
         total_expected, total_actual, missing_count, duplicate_count, 
         gap_count, quality_score)
        VALUES (:exchange, :symbol, :timeframe, :start_date, :end_date,
                :expected, :actual, :missing, :duplicates, :gaps, :quality_score)
        """
        
        await db.execute(query, {
            'exchange': exchange.lower(),
            'symbol': symbol,
            'timeframe': timeframe,
            'start_date': start_date,
            'end_date': end_date,
            'expected': completeness.get('expected_count', 0),
            'actual': completeness.get('actual_count', 0),
            'missing': completeness.get('missing_count', 0),
            'duplicates': duplication.get('total_duplicate_records', 0),
            'gaps': continuity.get('gap_count', 0),
            'quality_score': quality_score
        })
        
        await db.commit()

class DataCompletenessService:
    """æ•°æ®å®Œæ•´æ€§æœåŠ¡"""
    
    def __init__(self):
        self.quality_monitor = DataQualityMonitor()
    
    async def auto_repair_missing_data(
        self,
        exchange: str,
        symbol: str,
        timeframe: str,
        db: AsyncSession
    ) -> Dict[str, Any]:
        """è‡ªåŠ¨ä¿®å¤ç¼ºå¤±æ•°æ®"""
        
        logger.info(f"å¼€å§‹è‡ªåŠ¨ä¿®å¤æ•°æ®: {exchange} {symbol} {timeframe}")
        
        # æ£€æŸ¥æœ€è¿‘7å¤©æ•°æ®è´¨é‡
        quality_result = await self.quality_monitor.run_comprehensive_quality_check(
            exchange, symbol, timeframe, check_days=7, db=db
        )
        
        if quality_result['quality_score'] > 90:
            return {'success': True, 'message': 'æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ä¿®å¤'}
        
        # è·å–ç¼ºå¤±èŒƒå›´
        completeness = quality_result['completeness']
        if not completeness['is_complete']:
            # å°è¯•ä»å…¶ä»–æ•°æ®æºè¡¥å…¨
            repair_result = await self._repair_from_alternative_sources(
                db, exchange, symbol, timeframe, completeness
            )
            return repair_result
        
        return {'success': False, 'message': 'æ— æ³•è‡ªåŠ¨ä¿®å¤æ•°æ®'}
    
    async def _repair_from_alternative_sources(
        self,
        db: AsyncSession,
        exchange: str,
        symbol: str,
        timeframe: str,
        completeness: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä»å¤‡ç”¨æ•°æ®æºä¿®å¤æ•°æ®"""
        
        # å°è¯•ä»tickæ•°æ®èšåˆè¡¥å…¨
        from app.services.tick_data_manager import tick_to_kline_aggregator
        
        try:
            # è®¡ç®—ç¼ºå¤±æ—¶é—´èŒƒå›´
            missing_start = completeness.get('first_timestamp') or datetime.now() - timedelta(days=7)
            missing_end = completeness.get('last_timestamp') or datetime.now()
            
            # ä½¿ç”¨tickæ•°æ®å›å¡«
            backfilled_count = await tick_to_kline_aggregator.backfill_missing_klines(
                exchange, symbol, timeframe, [(missing_start, missing_end)], db
            )
            
            if backfilled_count > 0:
                return {
                    'success': True,
                    'message': f'æˆåŠŸä»tickæ•°æ®å›å¡« {backfilled_count} æ¡Kçº¿è®°å½•',
                    'backfilled_count': backfilled_count
                }
            else:
                return {
                    'success': False,
                    'message': 'æ— å¯ç”¨tickæ•°æ®è¿›è¡Œå›å¡«'
                }
                
        except Exception as e:
            logger.error(f"æ•°æ®ä¿®å¤å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'æ•°æ®ä¿®å¤å¤±è´¥: {str(e)}'
            }

# å…¨å±€å®ä¾‹
data_quality_monitor = DataQualityMonitor()
data_completeness_service = DataCompletenessService()