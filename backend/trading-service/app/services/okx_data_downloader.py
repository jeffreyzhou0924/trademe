"""
OKXæ•°æ®ä¸‹è½½å™¨
åŸºäº/root/Tradebotè„šæœ¬è®¾è®¡çš„ä¼ä¸šçº§OKXæ•°æ®ä¸‹è½½ç³»ç»Ÿ

åŠŸèƒ½ç‰¹æ€§ï¼š
1. Tickæ•°æ®ä¸‹è½½ - åŸºäºOKXå®˜æ–¹CDNçš„å†å²äº¤æ˜“æ•°æ®
2. Kçº¿æ•°æ®ä¸‹è½½ - åŸºäºOKX REST APIçš„å¤šå‘¨æœŸKçº¿æ•°æ®  
3. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ - åå°å¼‚æ­¥ä»»åŠ¡ç®¡ç†
4. æ•°æ®è´¨é‡æ§åˆ¶ - è‡ªåŠ¨å»é‡ã€æ ¼å¼éªŒè¯ã€ç¼ºå¤±æ£€æµ‹
5. è¿›åº¦ç›‘æ§ - å®æ—¶ä¸‹è½½è¿›åº¦å’ŒçŠ¶æ€è¿½è¸ª
"""

import asyncio
import aiohttp
import aiofiles
import zipfile
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
import logging
import gc
import psutil
import csv
import sys
from contextlib import asynccontextmanager

from app.database import AsyncSessionLocal
from app.services.okx_market_data_service import okx_market_service
from app.models.data_management import TickData
from app.models.data_collection import DataCollectionTask
from app.models.market_data import MarketData
from sqlalchemy import select, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime as dt

logger = logging.getLogger(__name__)


class ResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self, max_memory_percent: float = 80.0, max_cpu_percent: float = 85.0):
        self.max_memory_percent = max_memory_percent
        self.max_cpu_percent = max_cpu_percent
        self.process = psutil.Process()
    
    def get_memory_usage(self) -> float:
        """è·å–å†…å­˜ä½¿ç”¨ç‡"""
        return psutil.virtual_memory().percent
    
    def get_cpu_usage(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        return psutil.cpu_percent(interval=1)
    
    def get_process_memory_mb(self) -> float:
        """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def is_resource_available(self) -> tuple[bool, str]:
        """æ£€æŸ¥èµ„æºæ˜¯å¦å¯ç”¨"""
        memory_usage = self.get_memory_usage()
        cpu_usage = self.get_cpu_usage()
        
        if memory_usage > self.max_memory_percent:
            return False, f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_usage:.1f}% > {self.max_memory_percent}%"
        
        if cpu_usage > self.max_cpu_percent:
            return False, f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_usage:.1f}% > {self.max_cpu_percent}%"
        
        return True, "èµ„æºå……è¶³"
    
    async def wait_for_resources(self, max_wait_seconds: int = 300):
        """ç­‰å¾…èµ„æºå¯ç”¨"""
        start_time = datetime.now()
        
        while True:
            available, message = self.is_resource_available()
            if available:
                return
            
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > max_wait_seconds:
                raise RuntimeError(f"ç­‰å¾…èµ„æºè¶…æ—¶: {message}")
            
            logger.warning(f"èµ„æºä¸è¶³ï¼Œç­‰å¾…ä¸­... {message}")
            await asyncio.sleep(5)  # ç­‰å¾…5ç§’åé‡æ–°æ£€æŸ¥
    
    def force_cleanup(self):
        """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
        gc.collect()
        logger.info(f"å†…å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰è¿›ç¨‹å†…å­˜: {self.get_process_memory_mb():.1f}MB")


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class DataType(Enum):
    """æ•°æ®ç±»å‹"""
    TICK = "tick"
    KLINE = "kline"


@dataclass
class DownloadTask:
    """ä¸‹è½½ä»»åŠ¡"""
    task_id: str
    data_type: DataType
    exchange: str
    symbols: List[str]
    start_date: str
    end_date: str
    timeframes: List[str] = None  # Kçº¿ä¸“ç”¨
    status: TaskStatus = TaskStatus.PENDING
    progress: float = 0.0
    error_message: str = ""
    created_at: datetime = None
    started_at: datetime = None
    completed_at: datetime = None
    total_files: int = 0
    processed_files: int = 0
    downloaded_records: int = 0


class OKXDataDownloader:
    """OKXæ•°æ®ä¸‹è½½å™¨ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒèµ„æºç›‘æ§å’Œå†…å­˜ç®¡ç†"""
    
    def __init__(self):
        self.base_data_dir = Path("/root/trademe/backend/trading-service/data")
        self.okx_tick_dir = self.base_data_dir / "okx_tick_data"
        self.okx_kline_dir = self.base_data_dir / "okx_kline_data"
        self.temp_dir = self.base_data_dir / "temp"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.okx_tick_dir, self.okx_kline_dir, self.temp_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ä»»åŠ¡ç®¡ç†
        self.active_tasks: Dict[str, DownloadTask] = {}
        
        # èµ„æºç›‘æ§å™¨ - æ–°å¢
        self.resource_monitor = ResourceMonitor(max_memory_percent=75.0, max_cpu_percent=80.0)
        
        # ä¼˜åŒ–é…ç½®
        self.max_concurrent_downloads = 1  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…èµ„æºè¿‡è½½
        self.chunk_size = 500  # CSVå¤„ç†åˆ†å—å¤§å°
        self.batch_size = 50  # å‡å°‘æ‰¹é‡æ’å…¥å¤§å°ï¼Œé™ä½å†…å­˜å‹åŠ›
        self.max_files_per_task = 10  # é™åˆ¶å•ä¸ªä»»åŠ¡æœ€å¤§æ–‡ä»¶æ•°
        self.download_timeout = 30  # ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.retry_attempts = 2  # é‡è¯•æ¬¡æ•°
        
        # OKXé…ç½® - åŸºäºæ­£ç¡®çš„é“¾æ¥æ ¼å¼
        self.okx_tick_base_url = "https://www.okx.com/cdn/okex/traderecords/trades/daily"
        
        # æ”¯æŒçš„äº¤æ˜“å¯¹ - åŸºäºç°æœ‰è„šæœ¬é…ç½®
        self.supported_tick_symbols = [
            'BTC', 'ETH', 'LTC', 'XRP', 'BCH', 'SOL', 'BSV', 'TRB', 'AIDOGE', 'STARL',
            '1INCH', 'AAVE', 'ADA', 'AGLD', 'ALGO', 'ALPHA', 'ANT', 'APE', 'API3', 'APT',
            'AR', 'ARB', 'ATOM', 'AVAX', 'AXS', 'BADGER', 'BAL', 'BAND', 'BAT', 'BICO',
            'BIGTIME', 'BLUR', 'BNB', 'BNT', 'CELO', 'CEL', 'CETUS', 'CFX', 'CHZ', 'COMP',
            'CORE', 'CRO', 'CRV'
        ]
        
        self.supported_kline_symbols = [
            # ç°è´§äº¤æ˜“å¯¹
            'BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'ADA/USDT', 'DOT/USDT',
            'LINK/USDT', 'MATIC/USDT', 'AVAX/USDT', 'XRP/USDT', 'DOGE/USDT',
            # åˆçº¦äº¤æ˜“å¯¹ (æ°¸ç»­åˆçº¦)
            'BTC-USDT-SWAP', 'ETH-USDT-SWAP', 'SOL-USDT-SWAP', 'ADA-USDT-SWAP', 
            'DOT-USDT-SWAP', 'LINK-USDT-SWAP', 'MATIC-USDT-SWAP', 'AVAX-USDT-SWAP',
            'XRP-USDT-SWAP', 'DOGE-USDT-SWAP', 'LTC-USDT-SWAP', 'BCH-USDT-SWAP'
        ]
        
        self.supported_timeframes = ['1m', '5m', '15m', '30m', '1h', '2h', '4h', '1d', '1w']
        
        logger.info("ğŸš€ OKXæ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def create_tick_download_task(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        task_id: Optional[str] = None
    ) -> DownloadTask:
        """åˆ›å»ºTickæ•°æ®ä¸‹è½½ä»»åŠ¡ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡æŒä¹…åŒ–"""
        if not task_id:
            task_id = f"tick_{int(datetime.now().timestamp())}"
        
        # éªŒè¯äº¤æ˜“å¯¹
        invalid_symbols = [s for s in symbols if s not in self.supported_tick_symbols]
        if invalid_symbols:
            raise ValueError(f"ä¸æ”¯æŒçš„äº¤æ˜“å¯¹: {invalid_symbols}")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        date_range = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_range.append(current_dt.strftime('%Y%m%d'))
            current_dt += timedelta(days=1)
        
        total_files = len(symbols) * len(date_range)
        
        # æ£€æŸ¥æ–‡ä»¶æ•°é‡ï¼Œè¶…è¿‡é™åˆ¶åˆ™åˆ†å‰²ä»»åŠ¡
        if total_files > self.max_files_per_task:
            # é™åˆ¶æ—¥æœŸèŒƒå›´åˆ°æœ€è¿‘çš„å¤©æ•°
            max_days = self.max_files_per_task // len(symbols)
            if max_days < 1:
                max_days = 1
            
            # é‡æ–°è®¡ç®—æˆªæ­¢æ—¥æœŸ
            limited_end_dt = start_dt + timedelta(days=max_days-1)
            if limited_end_dt > end_dt:
                limited_end_dt = end_dt
            
            # æ›´æ–°æ—¥æœŸèŒƒå›´å’Œæ–‡ä»¶æ€»æ•°
            date_range = []
            current_dt = start_dt
            while current_dt <= limited_end_dt:
                date_range.append(current_dt.strftime('%Y%m%d'))
                current_dt += timedelta(days=1)
            
            total_files = len(symbols) * len(date_range)
            
            logger.info(f"âš ï¸ ä»»åŠ¡æ–‡ä»¶æ•°é‡è¿‡å¤šï¼Œé™åˆ¶ä¸º {total_files} ä¸ªæ–‡ä»¶ ({start_date}-{limited_end_dt.strftime('%Y%m%d')})")
        
        task = DownloadTask(
            task_id=task_id,
            data_type=DataType.TICK,
            exchange="okx",
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            status=TaskStatus.PENDING,
            created_at=datetime.now(),
            total_files=total_files
        )
        
        self.active_tasks[task_id] = task
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        await self._save_task_to_database(task)
        
        logger.info(f"ğŸ“ åˆ›å»ºTickæ•°æ®ä¸‹è½½ä»»åŠ¡: {task_id}, äº¤æ˜“å¯¹: {len(symbols)}, æ—¥æœŸ: {start_date}-{end_date}")
        
        return task
    
    async def create_kline_download_task(
        self,
        symbols: List[str],
        timeframes: List[str],
        start_date: str,
        end_date: str,
        task_id: Optional[str] = None
    ) -> DownloadTask:
        """åˆ›å»ºKçº¿æ•°æ®ä¸‹è½½ä»»åŠ¡ - æ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡æŒä¹…åŒ–"""
        if not task_id:
            task_id = f"kline_{int(datetime.now().timestamp())}"
        
        # éªŒè¯äº¤æ˜“å¯¹
        invalid_symbols = [s for s in symbols if s not in self.supported_kline_symbols]
        if invalid_symbols:
            raise ValueError(f"ä¸æ”¯æŒçš„äº¤æ˜“å¯¹: {invalid_symbols}")
        
        # éªŒè¯æ—¶é—´å‘¨æœŸ
        invalid_timeframes = [tf for tf in timeframes if tf not in self.supported_timeframes]
        if invalid_timeframes:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {invalid_timeframes}")
        
        total_files = len(symbols) * len(timeframes)
        
        task = DownloadTask(
            task_id=task_id,
            data_type=DataType.KLINE,
            exchange="okx",
            symbols=symbols,
            timeframes=timeframes,
            start_date=start_date,
            end_date=end_date,
            status=TaskStatus.PENDING,
            created_at=datetime.now(),
            total_files=total_files
        )
        
        self.active_tasks[task_id] = task
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
        await self._save_task_to_database(task)
        
        logger.info(f"ğŸ“ åˆ›å»ºKçº¿æ•°æ®ä¸‹è½½ä»»åŠ¡: {task_id}, äº¤æ˜“å¯¹: {len(symbols)}, æ—¶é—´å‘¨æœŸ: {timeframes}")
        
        return task
    
    async def execute_tick_download_task(self, task_id: str):
        """æ‰§è¡ŒTickæ•°æ®ä¸‹è½½ä»»åŠ¡ - æ”¹è¿›ç‰ˆæœ¬"""
        if task_id not in self.active_tasks:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        task = self.active_tasks[task_id]
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        await self._update_task_in_database(task)
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒTickä¸‹è½½ä»»åŠ¡: {task_id}")
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            start_dt = datetime.strptime(task.start_date, '%Y%m%d')
            end_dt = datetime.strptime(task.end_date, '%Y%m%d')
            
            total_downloaded = 0
            
            # æŒ‰æ—¥æœŸå¾ªç¯ä¸‹è½½
            current_dt = start_dt
            while current_dt <= end_dt:
                date_str = current_dt.strftime('%Y%m%d')
                formatted_date = current_dt.strftime('%Y-%m-%d')
                
                logger.info(f"ğŸ“… å¤„ç†æ—¥æœŸ: {formatted_date}")
                
                # é¡ºåºä¸‹è½½ï¼Œé¿å…èµ„æºè¿‡è½½ - ä¼˜åŒ–ï¼šæ”¹ä¸ºä¸²è¡Œå¤„ç†
                results = []
                for symbol in task.symbols:
                    try:
                        # èµ„æºæ£€æŸ¥ï¼šç­‰å¾…èµ„æºå¯ç”¨
                        await self.resource_monitor.wait_for_resources()
                        
                        logger.info(f"ğŸ” å¼€å§‹ä¸‹è½½: {symbol} (å†…å­˜: {self.resource_monitor.get_process_memory_mb():.1f}MB)")
                        
                        # ä¸²è¡Œä¸‹è½½ï¼Œé¿å…å¹¶å‘è¿‡è½½
                        result = await self._download_tick_file(symbol, date_str, formatted_date, task)
                        results.append(result)
                        
                        # å¼ºåˆ¶å†…å­˜æ¸…ç†
                        self.resource_monitor.force_cleanup()
                        
                        # é€‚å½“å»¶è¿Ÿï¼Œè®©ç³»ç»Ÿå–˜æ¯
                        await asyncio.sleep(1)  # å‡å°‘å»¶è¿Ÿæ—¶é—´
                        
                    except Exception as e:
                        logger.error(f"âŒ ä¸‹è½½å¤±è´¥ {symbol}: {e}")
                        results.append(e)
                
                # ç»Ÿè®¡ç»“æœ
                day_downloaded = 0
                for result in results:
                    if isinstance(result, Exception):
                        logger.error(f"âŒ ä¸‹è½½å‡ºé”™: {result}")
                        task.processed_files += 1  # è®¡ç®—å¤±è´¥çš„æ–‡ä»¶
                    else:
                        day_downloaded += result
                        total_downloaded += result
                        task.processed_files += 1
                    
                    task.progress = (task.processed_files / task.total_files) * 100
                
                logger.info(f"ğŸ“Š {formatted_date} å®Œæˆï¼Œå½“æ—¥ä¸‹è½½: {day_downloaded} æ¡è®°å½• (æ€»è®¡: {total_downloaded}), è¿›åº¦: {task.progress:.1f}%")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°æ•°æ®åº“
                task.downloaded_records = total_downloaded
                await self._update_task_in_database(task)
                
                current_dt += timedelta(days=1)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(2)
            
            task.downloaded_records = total_downloaded
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            
            # æœ€ç»ˆæ›´æ–°çŠ¶æ€
            await self._update_task_in_database(task)
            
            logger.info(f"âœ… Tickä¸‹è½½ä»»åŠ¡å®Œæˆ: {task_id}, ä¸‹è½½è®°å½•æ•°: {total_downloaded}")
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.completed_at = datetime.now()
            await self._update_task_in_database(task)
            logger.error(f"âŒ Tickä¸‹è½½ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {e}")
    
    async def _download_tick_file(self, symbol: str, date_str: str, formatted_date: str, task: DownloadTask) -> int:
        """ä¸‹è½½å•ä¸ªTickæ–‡ä»¶"""
        # æ„é€ OKXä¸‹è½½URL - åŸºäºæ­£ç¡®çš„é“¾æ¥æ ¼å¼
        # ç°è´§æ ¼å¼: BTC-USDT-trades-2025-08-29.zip
        # æ°¸ç»­åˆçº¦æ ¼å¼: BTC-USDT-SWAP-trades-2025-08-29.zip  
        # ä¼˜å…ˆå°è¯•æ°¸ç»­åˆçº¦æ•°æ®ï¼Œå› ä¸ºç°åœ¨OKXä¸»è¦æä¾›æ°¸ç»­åˆçº¦çš„tickæ•°æ®
        zip_filename = f"{symbol}-USDT-SWAP-trades-{formatted_date}.zip"  # æ°¸ç»­åˆçº¦æ ¼å¼
        download_url = f"{self.okx_tick_base_url}/{date_str}/{zip_filename}"
        
        temp_zip_path = self.temp_dir / zip_filename
        csv_filename = f"{symbol}-USDT-SWAP-trades-{formatted_date}.csv"
        temp_csv_path = self.temp_dir / csv_filename
        
        try:
            # æ³¨æ„ï¼šç°åœ¨æˆ‘ä»¬ä¸å†ä¿å­˜CSVæ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®ï¼Œç›´æ¥å¤„ç†ä¸´æ—¶æ–‡ä»¶
            # è¿™æ ·å¯ä»¥é¿å…é‡å¤å­˜å‚¨ï¼Œæ•°æ®å·²ç»åœ¨æ•°æ®åº“ä¸­äº†
            
            # ä¸‹è½½ZIPæ–‡ä»¶
            async with aiohttp.ClientSession() as session:
                logger.info(f"ğŸ“¥ ä¸‹è½½: {download_url}")
                
                async with session.get(download_url, timeout=300) as response:
                    if response.status == 200:
                        # ä¿å­˜ZIPæ–‡ä»¶
                        async with aiofiles.open(temp_zip_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(8192):
                                await f.write(chunk)
                        
                        # è§£å‹æ–‡ä»¶
                        try:
                            with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:
                                zip_ref.extractall(self.temp_dir)
                        except zipfile.BadZipFile:
                            logger.error(f"âŒ æ— æ•ˆçš„ZIPæ–‡ä»¶: {temp_zip_path}")
                            return 0
                        
                        # å¤„ç†CSVæ•°æ® - åŸºäºç°æœ‰è„šæœ¬çš„æ•°æ®å¤„ç†é€»è¾‘
                        processed_records = await self._process_tick_csv(temp_csv_path, symbol)
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if temp_zip_path.exists():
                            temp_zip_path.unlink()
                        if temp_csv_path.exists():
                            temp_csv_path.unlink()
                        
                        logger.info(f"âœ… {symbol} {formatted_date} å¤„ç†å®Œæˆï¼Œè®°å½•æ•°: {processed_records}")
                        return processed_records
                    
                    elif response.status == 404:
                        logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {download_url}")
                        return 0
                    else:
                        logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {download_url}, çŠ¶æ€ç : {response.status}")
                        return 0
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤„ç†å¤±è´¥: {zip_filename}, é”™è¯¯: {e}")
            # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
            for temp_file in [temp_zip_path, temp_csv_path]:
                if temp_file.exists():
                    temp_file.unlink()
            return 0
    
    async def _process_tick_csv(self, csv_path: Path, symbol: str) -> int:
        """å¤„ç†Tick CSVæ–‡ä»¶å¹¶æ’å…¥æ•°æ®åº“ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨åˆ†å—å¤„ç†é¿å…å†…å­˜è¿‡è½½"""
        try:
            if not csv_path.exists():
                return 0
            
            logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†tickæ•°æ®æ–‡ä»¶: {csv_path} (æ–‡ä»¶å¤§å°: {csv_path.stat().st_size / 1024 / 1024:.1f}MB)")
            
            # ä½¿ç”¨åŸç”ŸCSVè¯»å–å™¨æ›¿ä»£pandasï¼Œé¿å…æ•´ä¸ªæ–‡ä»¶åŠ è½½åˆ°å†…å­˜
            inserted_count = 0
            
            async with AsyncSessionLocal() as db:
                try:
                    # åˆ†å—å¤„ç†CSVæ–‡ä»¶ï¼Œé¿å…å†…å­˜è¿‡è½½
                    with open(csv_path, 'r', encoding='gbk', errors='ignore') as csvfile:
                        # å°è¯•æ£€æµ‹æ–‡ä»¶ç¼–ç 
                        try:
                            reader = csv.DictReader(csvfile)
                            headers = reader.fieldnames
                        except UnicodeDecodeError:
                            csvfile.close()
                            with open(csv_path, 'r', encoding='utf-8', errors='ignore') as csvfile:
                                reader = csv.DictReader(csvfile)
                                headers = reader.fieldnames
                    
                    # é‡æ–°æ‰“å¼€æ–‡ä»¶è¿›è¡Œåˆ†å—å¤„ç†
                    with open(csv_path, 'r', encoding='gbk', errors='ignore') as csvfile:
                        reader = csv.DictReader(csvfile)
                        
                        tick_records = []
                        processed_rows = 0
                        
                        for row in reader:
                            try:
                                # æ¯å¤„ç†ä¸€å®šæ•°é‡æ£€æŸ¥èµ„æº
                                if processed_rows % 1000 == 0:
                                    available, _ = self.resource_monitor.is_resource_available()
                                    if not available:
                                        logger.warning("âš ï¸ èµ„æºä¸è¶³ï¼Œæš‚åœå¤„ç†...")
                                        await asyncio.sleep(5)
                                
                                # åˆ›å»ºTickDataè®°å½•
                                tick_data = TickData(
                                    exchange="okx",
                                    symbol=f"{symbol}/USDT",
                                    price=float(row.get('price/ä»·æ ¼', 0) or row.get('price', 0)),
                                    volume=float(row.get('size/æ•°é‡', 0) or row.get('size', 0)),
                                    side=(row.get('side/äº¤æ˜“æ–¹å‘', '') or row.get('side', '')).lower(),
                                    trade_id=str(row.get('trade_id/æ’®åˆid', '') or row.get('trade_id', '')),
                                    timestamp=int(row.get('created_time/æˆäº¤æ—¶é—´', 0) or row.get('timestamp', 0)),
                                    data_source="okx_historical",
                                    created_at=datetime.now()
                                )
                                tick_records.append(tick_data)
                                processed_rows += 1
                                
                                # åˆ†æ‰¹æ’å…¥æ•°æ®åº“ï¼Œé¿å…å•æ¬¡äº‹åŠ¡è¿‡å¤§
                                if len(tick_records) >= self.batch_size:
                                    # å»é‡æ£€æŸ¥ï¼šæŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è®°å½•
                                    duplicates_removed = await self._remove_duplicates_tick(db, tick_records)
                                    if duplicates_removed:
                                        db.add_all(duplicates_removed)
                                        await db.commit()
                                        inserted_count += len(duplicates_removed)
                                        skipped_count = len(tick_records) - len(duplicates_removed)
                                        if skipped_count > 0:
                                            logger.info(f"âš ï¸ è·³è¿‡é‡å¤è®°å½•: {skipped_count} æ¡")
                                    
                                    logger.debug(f"ğŸ”„ å·²æ’å…¥ {inserted_count} æ¡è®°å½• (å†…å­˜: {self.resource_monitor.get_process_memory_mb():.1f}MB)")
                                    
                                    tick_records.clear()  # æ¸…ç©ºåˆ—è¡¨é‡Šæ”¾å†…å­˜
                                    
                                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                                    if inserted_count % (self.batch_size * 10) == 0:
                                        gc.collect()
                                
                            except Exception as row_error:
                                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {processed_rows}: {row_error}")
                                continue
                        
                        # å¤„ç†å‰©ä½™çš„è®°å½•
                        if tick_records:
                            duplicates_removed = await self._remove_duplicates_tick(db, tick_records)
                            if duplicates_removed:
                                db.add_all(duplicates_removed)
                                await db.commit()
                                inserted_count += len(duplicates_removed)
                                skipped_count = len(tick_records) - len(duplicates_removed)
                                if skipped_count > 0:
                                    logger.info(f"âš ï¸ æœ€ç»ˆæ‰¹æ¬¡è·³è¿‡é‡å¤è®°å½•: {skipped_count} æ¡")
                    
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} æ¡tickæ•°æ®åˆ°æ•°æ®åº“")
                    
                    # æ•°æ®å·²æˆåŠŸæ’å…¥æ•°æ®åº“ï¼Œåˆ é™¤CSVæ–‡ä»¶ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´
                    if csv_path.exists():
                        csv_path.unlink()
                        logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤å¤„ç†å®Œæˆçš„CSVæ–‡ä»¶: {csv_path.name}")
                    
                    return inserted_count
                    
                except Exception as db_error:
                    await db.rollback()
                    logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {db_error}")
                    # æ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œåˆ é™¤CSVæ–‡ä»¶ï¼Œé¿å…å ç”¨å­˜å‚¨ç©ºé—´
                    if csv_path.exists():
                        csv_path.unlink()
                        logger.warning(f"âš ï¸ æ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œå·²åˆ é™¤CSVæ–‡ä»¶: {csv_path.name}")
                    return 0
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†tick CSVå¤±è´¥: {csv_path}, é”™è¯¯: {e}")
            return 0
    
    async def execute_kline_download_task(self, task_id: str):
        """æ‰§è¡ŒKçº¿æ•°æ®ä¸‹è½½ä»»åŠ¡ - æ”¹è¿›ç‰ˆæœ¬"""
        if task_id not in self.active_tasks:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        task = self.active_tasks[task_id]
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        await self._update_task_in_database(task)
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒKçº¿ä¸‹è½½ä»»åŠ¡: {task_id}")
            
            start_dt = datetime.strptime(task.start_date, '%Y%m%d')
            end_dt = datetime.strptime(task.end_date, '%Y%m%d')
            
            total_downloaded = 0
            
            # æŒ‰äº¤æ˜“å¯¹å’Œæ—¶é—´å‘¨æœŸå¾ªç¯ä¸‹è½½
            for symbol in task.symbols:
                for timeframe in task.timeframes:
                    try:
                        logger.info(f"ğŸ“ˆ å¼€å§‹ä¸‹è½½: {symbol} {timeframe}")
                        
                        # ä½¿ç”¨æ”¹è¿›çš„Kçº¿ä¸‹è½½æ–¹æ³•
                        records = await self._download_kline_data(
                            symbol, timeframe, start_dt, end_dt
                        )
                        
                        total_downloaded += records
                        task.processed_files += 1
                        task.progress = (task.processed_files / task.total_files) * 100
                        
                        logger.info(f"âœ… {symbol} {timeframe} ä¸‹è½½å®Œæˆï¼Œè®°å½•æ•°: {records}, è¿›åº¦: {task.progress:.1f}%")
                        
                        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                        task.downloaded_records = total_downloaded
                        await self._update_task_in_database(task)
                        
                        # é¿å…APIé™æµ
                        await asyncio.sleep(1)
                        
                    except Exception as e:
                        logger.error(f"âŒ {symbol} {timeframe} ä¸‹è½½å¤±è´¥: {e}")
                        task.processed_files += 1
                        task.progress = (task.processed_files / task.total_files) * 100
                        await self._update_task_in_database(task)
            
            task.downloaded_records = total_downloaded
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            
            # æœ€ç»ˆæ›´æ–°çŠ¶æ€
            await self._update_task_in_database(task)
            
            logger.info(f"âœ… Kçº¿ä¸‹è½½ä»»åŠ¡å®Œæˆ: {task_id}, ä¸‹è½½è®°å½•æ•°: {total_downloaded}")
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.completed_at = datetime.now()
            await self._update_task_in_database(task)
            logger.error(f"âŒ Kçº¿ä¸‹è½½ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {e}")
    
    async def _download_kline_data(
        self, 
        symbol: str, 
        timeframe: str, 
        start_dt: datetime, 
        end_dt: datetime
    ) -> int:
        """ä¸‹è½½Kçº¿æ•°æ® - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿æ•°æ®å®é™…æ’å…¥æ•°æ®åº“"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½Kçº¿æ•°æ®: {symbol} {timeframe} ({start_dt.date()} åˆ° {end_dt.date()})")
            
            total_records = 0
            saved_records = 0
            current_end = int(end_dt.timestamp() * 1000)
            start_timestamp = int(start_dt.timestamp() * 1000)
            
            async with AsyncSessionLocal() as db:
                # è®¡ç®—åˆé€‚çš„æ¯æ¬¡è¯·æ±‚æ•°é‡ - åŸºäºæ—¶é—´æ¡†æ¶
                # 1åˆ†é’Ÿ: 300æ¡çº¦ç­‰äº5å°æ—¶, 1å°æ—¶: 300æ¡çº¦ç­‰äº12.5å¤©
                request_limit = 300  # OKX APIå•æ¬¡æœ€å¤§é™åˆ¶
                
                # åˆ†æ‰¹è¯·æ±‚å†å²æ•°æ® - ä½¿ç”¨æ—¶é—´èŒƒå›´åˆ†é¡µ
                request_count = 0
                while current_end > start_timestamp and request_count < 50:  # å‡å°‘æœ€å¤§è¯·æ±‚æ¬¡æ•°ï¼Œä¾é æ—¶é—´èŒƒå›´æ§åˆ¶
                    try:
                        # ğŸ†• ä½¿ç”¨æ—¶é—´èŒƒå›´å‚æ•°è·å–Kçº¿æ•°æ®
                        kline_data = await okx_market_service.get_klines(
                            symbol=symbol,
                            timeframe=timeframe,
                            limit=request_limit,
                            start_time=start_timestamp,  # ğŸ†• å¼€å§‹æ—¶é—´
                            end_time=current_end,        # ğŸ†• ç»“æŸæ—¶é—´
                            use_cache=False
                        )
                        
                        if not kline_data.get('klines') or len(kline_data['klines']) == 0:
                            logger.warning(f"âš ï¸ æ²¡æœ‰æ›´å¤šKçº¿æ•°æ®: {symbol} {timeframe}")
                            break
                        
                        new_records = len(kline_data['klines'])
                        total_records += new_records
                        request_count += 1
                        
                        # ğŸ†• å°†Kçº¿æ•°æ®è½¬æ¢ä¸ºMarketDataå¯¹è±¡å¹¶ä¿å­˜åˆ°æ•°æ®åº“
                        market_data_records = []
                        for kline in kline_data['klines']:
                            # klineæ ¼å¼: [timestamp, open, high, low, close, volume, ...]
                            try:
                                timestamp_ms = int(kline[0])
                                dt_obj = dt.fromtimestamp(timestamp_ms / 1000)
                                
                                product_type = 'futures' if symbol.upper().endswith('-USDT-SWAP') or symbol.upper().endswith('-SWAP') else 'spot'
                                market_record = MarketData(
                                    exchange="okx",
                                    symbol=symbol,
                                    timeframe=timeframe,
                                    product_type=product_type,
                                    open_price=float(kline[1]),
                                    high_price=float(kline[2]), 
                                    low_price=float(kline[3]),
                                    close_price=float(kline[4]),
                                    volume=float(kline[5]),
                                    timestamp=dt_obj
                                )
                                market_data_records.append(market_record)
                            except (ValueError, IndexError) as parse_error:
                                logger.warning(f"âš ï¸ è§£æKçº¿æ•°æ®å¤±è´¥: {kline}, é”™è¯¯: {parse_error}")
                                continue
                        
                        # å»é‡é€»è¾‘ï¼šæ£€æŸ¥ç°æœ‰è®°å½•é¿å…é‡å¤
                        if market_data_records:
                            # æ£€æŸ¥é‡å¤è®°å½•
                            existing_timestamps = []
                            for record in market_data_records:
                                existing_query = await db.execute(
                                    select(MarketData.timestamp).where(
                                        and_(
                                            MarketData.exchange == "okx",
                                            MarketData.symbol == symbol,
                                            MarketData.timeframe == timeframe,
                                            MarketData.timestamp == record.timestamp
                                        )
                                    )
                                )
                                if existing_query.scalar_one_or_none():
                                    existing_timestamps.append(record.timestamp)
                            
                            # è¿‡æ»¤æ‰é‡å¤è®°å½•
                            unique_records = [record for record in market_data_records 
                                            if record.timestamp not in existing_timestamps]
                            
                            if unique_records:
                                db.add_all(unique_records)
                                await db.commit()
                                saved_records += len(unique_records)
                                logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(unique_records)} æ¡Kçº¿æ•°æ®åˆ°æ•°æ®åº“ (è·³è¿‡é‡å¤: {len(market_data_records) - len(unique_records)} æ¡)")
                            else:
                                logger.info(f"âš ï¸ æœ¬æ‰¹æ¬¡æ‰€æœ‰ {len(market_data_records)} æ¡è®°å½•éƒ½æ˜¯é‡å¤æ•°æ®")
                        
                        logger.info(f"ğŸ“Š è·å– {new_records} æ¡Kçº¿æ•°æ® (æ€»è®¡: {total_records}, å·²ä¿å­˜: {saved_records}, è¯·æ±‚æ¬¡æ•°: {request_count})")
                        
                        # æ›´æ–°æ—¶é—´èŒƒå›´ç”¨äºä¸‹ä¸€æ¬¡è¯·æ±‚
                        if kline_data['klines']:
                            earliest_timestamp = kline_data['klines'][0][0]  # æœ€æ—©çš„æ—¶é—´æˆ³
                            latest_timestamp = kline_data['klines'][-1][0]   # æœ€æ–°çš„æ—¶é—´æˆ³
                            
                            # æ£€æŸ¥æ—¶é—´è¾¹ç•Œ
                            if earliest_timestamp <= start_timestamp:
                                logger.info(f"âœ… å·²åˆ°è¾¾èµ·å§‹æ—¶é—´: {datetime.fromtimestamp(start_timestamp/1000)} (è·å¾—: {datetime.fromtimestamp(earliest_timestamp/1000)})")
                                break
                            
                            # ä¸‹æ¬¡è¯·æ±‚æ›´æ—©çš„æ•°æ® - ä»æœ€æ—©çš„æ—¶é—´æˆ³ä¹‹å‰å¼€å§‹
                            current_end = earliest_timestamp - 1
                            logger.debug(f"ğŸ”„ ä¸‹ä¸€æ¬¡è¯·æ±‚æ—¶é—´èŒƒå›´: {datetime.fromtimestamp(start_timestamp/1000)} åˆ° {datetime.fromtimestamp(current_end/1000)}")
                        else:
                            logger.warning(f"âš ï¸ æ²¡æœ‰Kçº¿æ•°æ®è¿”å›")
                            break
                        
                        # å¦‚æœè·å–çš„è®°å½•å°‘äº300æ¡ï¼Œè¯´æ˜å·²ç»åˆ°è¾¾å†å²æ•°æ®è¾¹ç•Œ
                        if new_records < 300:
                            logger.info(f"âœ… å·²åˆ°è¾¾å†å²æ•°æ®è¾¹ç•Œï¼Œè®°å½•æ•°: {new_records}")
                            break
                        
                        # é¿å…APIé™æµ
                        await asyncio.sleep(0.3)
                        
                    except Exception as request_error:
                        logger.error(f"âŒ å•æ¬¡è¯·æ±‚å¤±è´¥: {request_error}")
                        await asyncio.sleep(1)  # å‡ºé”™æ—¶ç­‰å¾…æ›´ä¹…
                        request_count += 1
                        continue
            
            logger.info(f"âœ… Kçº¿ä¸‹è½½å®Œæˆ: {symbol} {timeframe}, æ€»ä¸‹è½½è®°å½•æ•°: {total_records}, å®é™…ä¿å­˜åˆ°æ•°æ®åº“: {saved_records}, è¯·æ±‚æ¬¡æ•°: {request_count}")
            return saved_records  # è¿”å›å®é™…ä¿å­˜çš„è®°å½•æ•°
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½Kçº¿æ•°æ®å¤±è´¥: {symbol} {timeframe}, é”™è¯¯: {e}")
            return 0
    
    async def get_task_status(self, task_id: str) -> Optional[DownloadTask]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.active_tasks.get(task_id)
    
    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        if task_id in self.active_tasks:
            task = self.active_tasks[task_id]
            if task.status == TaskStatus.RUNNING:
                task.status = TaskStatus.CANCELLED
                task.completed_at = datetime.now()
                logger.info(f"â¹ï¸ ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
                return True
        return False
    
    async def list_active_tasks(self) -> List[DownloadTask]:
        """åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä»»åŠ¡"""
        return list(self.active_tasks.values())
    
    async def clean_completed_tasks(self, days_old: int = 7):
        """æ¸…ç†å®Œæˆçš„ä»»åŠ¡"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        tasks_to_remove = []
        
        for task_id, task in self.active_tasks.items():
            if (task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED] 
                and task.completed_at 
                and task.completed_at < cutoff_date):
                tasks_to_remove.append(task_id)
        
        for task_id in tasks_to_remove:
            del self.active_tasks[task_id]
        
        logger.info(f"ğŸ§¹ æ¸…ç†å®Œæˆä»»åŠ¡: {len(tasks_to_remove)} ä¸ª")
    
    async def _save_task_to_database(self, task: DownloadTask):
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        try:
            async with AsyncSessionLocal() as db:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
                existing_task = await db.execute(
                    select(DataCollectionTask).where(
                        DataCollectionTask.task_name == task.task_id
                    )
                )
                
                if existing_task.scalar_one_or_none():
                    logger.warning(f"âš ï¸ ä»»åŠ¡å·²å­˜åœ¨åœ¨æ•°æ®åº“ä¸­: {task.task_id}")
                    return
                
                # åˆ›å»ºæ–°ä»»åŠ¡è®°å½•
                db_task = DataCollectionTask(
                    task_name=task.task_id,
                    exchange=task.exchange,
                    data_type=task.data_type.value,
                    symbols=','.join(task.symbols),
                    timeframes=','.join(task.timeframes) if task.timeframes else None,
                    status=task.status.value,
                    schedule_type="manual",
                    success_count=0,
                    error_count=0,
                    total_records=0,
                    created_at=task.created_at,
                    config=f"{{\"start_date\": \"{task.start_date}\", \"end_date\": \"{task.end_date}\", \"total_files\": {task.total_files}}}"
                )
                
                db.add(db_task)
                await db.commit()
                logger.info(f"ğŸ’¾ ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“: {task.task_id}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    async def _update_task_in_database(self, task: DownloadTask):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€"""
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(DataCollectionTask).where(
                        DataCollectionTask.task_name == task.task_id
                    )
                )
                
                db_task = result.scalar_one_or_none()
                if db_task:
                    db_task.status = task.status.value
                    db_task.success_count = task.processed_files
                    db_task.total_records = task.downloaded_records
                    db_task.last_run_at = task.started_at
                    if task.completed_at:
                        db_task.next_run_at = task.completed_at
                    if task.error_message:
                        db_task.last_error_message = task.error_message
                        db_task.last_error_at = datetime.now()
                    
                    await db.commit()
                    logger.debug(f"ğŸ”„ ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°: {task.task_id} -> {task.status.value}")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
    
    async def get_download_statistics(self) -> Dict[str, Any]:
        """è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_tasks": len(self.active_tasks),
            "running_tasks": len([t for t in self.active_tasks.values() if t.status == TaskStatus.RUNNING]),
            "completed_tasks": len([t for t in self.active_tasks.values() if t.status == TaskStatus.COMPLETED]),
            "failed_tasks": len([t for t in self.active_tasks.values() if t.status == TaskStatus.FAILED]),
            "total_downloaded_records": sum(t.downloaded_records for t in self.active_tasks.values() if t.downloaded_records),
            "tick_data_db_records": "check database for current count",
            "supported_tick_symbols": len(self.supported_tick_symbols),
            "supported_kline_symbols": len(self.supported_kline_symbols)
        }
        
        # ä»æ•°æ®åº“è·å–æ›´å‡†ç¡®çš„ç»Ÿè®¡
        try:
            async with AsyncSessionLocal() as db:
                # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ä»»åŠ¡
                db_tasks_result = await db.execute(select(DataCollectionTask))
                db_tasks = db_tasks_result.scalars().all()
                
                stats["database_tasks"] = len(db_tasks)
                stats["database_completed"] = len([t for t in db_tasks if t.status == "completed"])
                stats["database_failed"] = len([t for t in db_tasks if t.status == "failed"])
                
                # æŸ¥è¯¢tickæ•°æ®æ€»æ•°
                from app.models.data_management import TickData
                tick_count_result = await db.execute(select(TickData))
                tick_count = len(tick_count_result.scalars().all())
                stats["tick_records_in_db"] = tick_count
                
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
        
        return stats

    async def _remove_duplicates_tick(self, db: AsyncSession, tick_records: List[TickData]) -> List[TickData]:
        """ç§»é™¤é‡å¤çš„Tickæ•°æ®è®°å½•"""
        if not tick_records:
            return []
        
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼šä½¿ç”¨symbol, timestamp, trade_idä½œä¸ºå”¯ä¸€æ€§åˆ¤æ–­
            unique_keys = [(record.symbol, record.timestamp, record.trade_id) for record in tick_records]
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è®°å½•
            existing_query = select(TickData.symbol, TickData.timestamp, TickData.trade_id).where(
                or_(*[
                    and_(
                        TickData.symbol == symbol,
                        TickData.timestamp == timestamp,
                        TickData.trade_id == trade_id
                    ) for symbol, timestamp, trade_id in unique_keys
                ])
            )
            
            result = await db.execute(existing_query)
            existing_keys = set(result.fetchall())
            
            # è¿‡æ»¤æ‰é‡å¤è®°å½•
            unique_records = []
            for record in tick_records:
                key = (record.symbol, record.timestamp, record.trade_id)
                if key not in existing_keys:
                    unique_records.append(record)
            
            return unique_records
            
        except Exception as e:
            logger.error(f"âŒ å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return tick_records  # å¦‚æœå»é‡å¤±è´¥ï¼Œè¿”å›åŸè®°å½•
    
    async def _remove_duplicates_kline(self, db: AsyncSession, market_records: List) -> List:
        """ç§»é™¤é‡å¤çš„Kçº¿æ•°æ®è®°å½•"""
        if not market_records:
            return []
        
        try:
            from app.models.market_data import MarketData
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼šä½¿ç”¨exchange, symbol, timeframe, timestampä½œä¸ºå”¯ä¸€æ€§åˆ¤æ–­
            unique_keys = [(record.exchange, record.symbol, record.timeframe, record.timestamp) 
                          for record in market_records if hasattr(record, 'exchange')]
            
            if not unique_keys:
                return market_records
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è®°å½•
            existing_query = select(MarketData.exchange, MarketData.symbol, MarketData.timeframe, MarketData.timestamp).where(
                or_(*[
                    and_(
                        MarketData.exchange == exchange,
                        MarketData.symbol == symbol,
                        MarketData.timeframe == timeframe,
                        MarketData.timestamp == timestamp
                    ) for exchange, symbol, timeframe, timestamp in unique_keys
                ])
            )
            
            result = await db.execute(existing_query)
            existing_keys = set(result.fetchall())
            
            # è¿‡æ»¤æ‰é‡å¤è®°å½•
            unique_records = []
            for record in market_records:
                if hasattr(record, 'exchange'):
                    key = (record.exchange, record.symbol, record.timeframe, record.timestamp)
                    if key not in existing_keys:
                        unique_records.append(record)
                else:
                    unique_records.append(record)
            
            return unique_records
            
        except Exception as e:
            logger.error(f"âŒ Kçº¿å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return market_records  # å¦‚æœå»é‡å¤±è´¥ï¼Œè¿”å›åŸè®°å½•




# å…¨å±€å®ä¾‹
okx_data_downloader = OKXDataDownloader()
