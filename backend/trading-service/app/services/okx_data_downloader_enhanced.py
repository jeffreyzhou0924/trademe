"""
OKXæ•°æ®ä¸‹è½½å™¨ - å¢å¼ºç‰ˆæœ¬
åŸºäºç°æœ‰é—®é¢˜çš„ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼ŒåŒ…å«ï¼š
1. ç²¾ç¡®çš„ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
2. æ•°æ®å®Œæ•´æ€§éªŒè¯
3. æ™ºèƒ½é‡è¯•æœºåˆ¶
4. è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
5. åˆ†æ—¶é—´æ®µä¸‹è½½ç­–ç•¥
"""

import asyncio
import aiohttp
import aiofiles
import zipfile
import os
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
import logging
import gc
import psutil
import csv
import sys
import json
from contextlib import asynccontextmanager

from app.database import AsyncSessionLocal
from app.services.okx_market_data_service import okx_market_service
from app.models.data_management import TickData
from app.models.data_collection import DataCollectionTask
from app.models.market_data import MarketData
from sqlalchemy import select, and_, or_, func, text
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime as dt

logger = logging.getLogger(__name__)


class TaskStatus(Enum):
    """å¢å¼ºçš„ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running" 
    COMPLETED = "completed"
    PARTIAL_SUCCESS = "partial_success"  # æ–°å¢ï¼šéƒ¨åˆ†æˆåŠŸ
    FAILED = "failed"
    CANCELLED = "cancelled"


class DataType(Enum):
    """æ•°æ®ç±»å‹"""
    TICK = "tick"
    KLINE = "kline"


@dataclass
class SubTaskProgress:
    """å­ä»»åŠ¡è¿›åº¦è·Ÿè¸ª"""
    symbol: str
    timeframe: str = ""
    total_expected_records: int = 0
    actual_records: int = 0
    status: TaskStatus = TaskStatus.PENDING
    error_message: str = ""
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    retry_count: int = 0
    data_quality_score: float = 0.0  # æ•°æ®è´¨é‡è¯„åˆ†


@dataclass 
class EnhancedDownloadTask:
    """å¢å¼ºçš„ä¸‹è½½ä»»åŠ¡"""
    task_id: str
    data_type: DataType
    exchange: str
    symbols: List[str]
    start_date: str
    end_date: str
    timeframes: List[str] = None
    status: TaskStatus = TaskStatus.PENDING
    progress: float = 0.0
    error_message: str = ""
    created_at: datetime = None
    started_at: datetime = None
    completed_at: datetime = None
    total_files: int = 0
    processed_files: int = 0
    downloaded_records: int = 0
    expected_records: int = 0  # æ–°å¢ï¼šé¢„æœŸè®°å½•æ•°
    
    # æ–°å¢ï¼šè¯¦ç»†çš„å­ä»»åŠ¡è·Ÿè¸ª
    subtasks: List[SubTaskProgress] = field(default_factory=list)
    failed_subtasks: List[SubTaskProgress] = field(default_factory=list)
    quality_issues: List[str] = field(default_factory=list)
    
    # æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡
    average_download_speed: float = 0.0  # records/second
    peak_memory_usage: float = 0.0  # MB
    total_api_requests: int = 0
    failed_api_requests: int = 0


class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    @staticmethod
    async def check_kline_data_quality(
        db: AsyncSession, 
        symbol: str, 
        timeframe: str, 
        start_date: datetime, 
        end_date: datetime
    ) -> Tuple[float, List[str]]:
        """æ£€æŸ¥Kçº¿æ•°æ®è´¨é‡"""
        issues = []
        quality_score = 100.0
        
        try:
            # 1. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ - è®¡ç®—é¢„æœŸè®°å½•æ•°
            expected_records = DataQualityChecker._calculate_expected_kline_records(
                timeframe, start_date, end_date
            )
            
            # æŸ¥è¯¢å®é™…è®°å½•æ•°
            result = await db.execute(
                select(func.count(MarketData.id)).where(
                    and_(
                        MarketData.exchange == "okx",
                        MarketData.symbol == symbol,
                        MarketData.timeframe == timeframe,
                        MarketData.timestamp >= start_date,
                        MarketData.timestamp <= end_date
                    )
                )
            )
            actual_records = result.scalar() or 0
            
            # è®¡ç®—å®Œæ•´æ€§å¾—åˆ†
            if expected_records > 0:
                completeness = min(actual_records / expected_records, 1.0)
                quality_score *= completeness
                
                if completeness < 0.95:
                    issues.append(f"æ•°æ®ä¸å®Œæ•´: é¢„æœŸ {expected_records} æ¡ï¼Œå®é™… {actual_records} æ¡ (å®Œæ•´åº¦: {completeness:.1%})")
            
            # 2. æ£€æŸ¥æ•°æ®è¿ç»­æ€§ - æŸ¥æ‰¾æ—¶é—´é—´éš™
            gaps_result = await db.execute(
                text("""
                SELECT COUNT(*) as gap_count FROM (
                    SELECT 
                        timestamp,
                        LAG(timestamp) OVER (ORDER BY timestamp) as prev_timestamp
                    FROM market_data 
                    WHERE exchange = :exchange AND symbol = :symbol AND timeframe = :timeframe
                        AND timestamp BETWEEN :start_date AND :end_date
                ) t
                WHERE JULIANDAY(timestamp) - JULIANDAY(prev_timestamp) > :expected_interval
                """),
                {
                    "exchange": "okx",
                    "symbol": symbol, 
                    "timeframe": timeframe,
                    "start_date": start_date,
                    "end_date": end_date,
                    "expected_interval": DataQualityChecker._get_timeframe_days(timeframe) * 2  # å…è®¸2å€çš„é—´éš”å®¹å·®
                }
            )
            gap_count = gaps_result.scalar() or 0
            
            if gap_count > 0:
                quality_score *= max(0.7, 1.0 - (gap_count * 0.1))
                issues.append(f"å‘ç° {gap_count} ä¸ªæ—¶é—´é—´éš™")
            
            # 3. æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
            price_check_result = await db.execute(
                select(
                    func.count().filter(MarketData.open_price <= 0).label('invalid_price_count'),
                    func.count().filter(MarketData.high_price < MarketData.low_price).label('invalid_range_count')
                ).where(
                    and_(
                        MarketData.exchange == "okx",
                        MarketData.symbol == symbol,
                        MarketData.timeframe == timeframe,
                        MarketData.timestamp >= start_date,
                        MarketData.timestamp <= end_date
                    )
                )
            )
            
            price_issues = price_check_result.first()
            if price_issues.invalid_price_count > 0:
                quality_score *= 0.8
                issues.append(f"å‘ç° {price_issues.invalid_price_count} æ¡æ— æ•ˆä»·æ ¼è®°å½•")
                
            if price_issues.invalid_range_count > 0:
                quality_score *= 0.9
                issues.append(f"å‘ç° {price_issues.invalid_range_count} æ¡ä»·æ ¼èŒƒå›´å¼‚å¸¸è®°å½•")
            
            return quality_score, issues
            
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return 50.0, [f"è´¨é‡æ£€æŸ¥å¼‚å¸¸: {str(e)}"]
    
    @staticmethod
    def _calculate_expected_kline_records(timeframe: str, start_date: datetime, end_date: datetime) -> int:
        """è®¡ç®—é¢„æœŸçš„Kçº¿è®°å½•æ•°"""
        total_days = (end_date - start_date).days + 1
        
        timeframe_minutes = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30,
            '1h': 60, '2h': 120, '4h': 240,
            '1d': 1440, '1w': 10080
        }
        
        if timeframe not in timeframe_minutes:
            return 0
        
        minutes_per_record = timeframe_minutes[timeframe]
        records_per_day = 1440 / minutes_per_record  # ä¸€å¤©çš„åˆ†é’Ÿæ•° / æ¯æ¡è®°å½•çš„åˆ†é’Ÿæ•°
        
        # è€ƒè™‘å¸‚åœºä¼‘å¸‚æ—¶é—´ (å‡è®¾åŠ å¯†è´§å¸å¸‚åœº24å°æ—¶äº¤æ˜“)
        return int(total_days * records_per_day * 0.99)  # 99% é¢„æœŸå®Œæ•´åº¦ï¼Œè€ƒè™‘ç»´æŠ¤æ—¶é—´
    
    @staticmethod
    def _get_timeframe_days(timeframe: str) -> float:
        """è·å–æ—¶é—´æ¡†æ¶å¯¹åº”çš„å¤©æ•°"""
        timeframe_days = {
            '1m': 1/1440, '5m': 5/1440, '15m': 15/1440, '30m': 30/1440,
            '1h': 1/24, '2h': 2/24, '4h': 4/24,
            '1d': 1, '1w': 7
        }
        return timeframe_days.get(timeframe, 1/1440)


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 2.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    async def execute_with_retry(self, coro_func, *args, **kwargs):
        """å¸¦é‡è¯•çš„æ‰§è¡Œå™¨"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await coro_func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries:
                    delay = self.base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"é‡è¯• {self.max_retries} æ¬¡åä»å¤±è´¥: {e}")
        
        raise last_exception


class EnhancedOKXDataDownloader:
    """å¢å¼ºç‰ˆOKXæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.base_data_dir = Path("/root/trademe/backend/trading-service/data")
        self.okx_tick_dir = self.base_data_dir / "okx_tick_data"
        self.okx_kline_dir = self.base_data_dir / "okx_kline_data"
        self.temp_dir = self.base_data_dir / "temp"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.okx_tick_dir, self.okx_kline_dir, self.temp_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ä»»åŠ¡ç®¡ç†
        self.active_tasks: Dict[str, EnhancedDownloadTask] = {}
        
        # å¢å¼ºç»„ä»¶
        self.quality_checker = DataQualityChecker()
        self.retry_manager = RetryManager(max_retries=3, base_delay=2.0)
        
        # é…ç½®
        self.max_concurrent_downloads = 1
        self.chunk_size = 500
        self.batch_size = 100
        
        # OKXé…ç½®
        self.okx_tick_base_url = "https://static.okx.com/cdn/okex/traderecords/trades/daily"
        
        self.supported_tick_symbols = [
            'BTC', 'ETH', 'LTC', 'XRP', 'BCH', 'SOL', 'BSV', 'TRB', 'AIDOGE', 'STARL',
            '1INCH', 'AAVE', 'ADA', 'AGLD', 'ALGO', 'ALPHA', 'ANT', 'APE', 'API3', 'APT',
        ]
        
        self.supported_kline_symbols = [
            'BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'ADA/USDT', 'DOT/USDT',
            'BTC-USDT-SWAP', 'ETH-USDT-SWAP', 'SOL-USDT-SWAP', 'ADA-USDT-SWAP', 
            'DOT-USDT-SWAP', 'LINK-USDT-SWAP', 'MATIC-USDT-SWAP', 'AVAX-USDT-SWAP',
        ]
        
        self.supported_timeframes = ['1m', '5m', '15m', '30m', '1h', '2h', '4h', '1d', '1w']
        
        logger.info("ğŸš€ å¢å¼ºç‰ˆOKXæ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def create_enhanced_kline_task(
        self,
        symbols: List[str],
        timeframes: List[str], 
        start_date: str,
        end_date: str,
        task_id: Optional[str] = None
    ) -> EnhancedDownloadTask:
        """åˆ›å»ºå¢å¼ºçš„Kçº¿ä¸‹è½½ä»»åŠ¡"""
        if not task_id:
            task_id = f"enhanced_kline_{int(datetime.now().timestamp())}"
        
        # éªŒè¯å‚æ•°
        invalid_symbols = [s for s in symbols if s not in self.supported_kline_symbols]
        if invalid_symbols:
            raise ValueError(f"ä¸æ”¯æŒçš„äº¤æ˜“å¯¹: {invalid_symbols}")
        
        invalid_timeframes = [tf for tf in timeframes if tf not in self.supported_timeframes]
        if invalid_timeframes:
            raise ValueError(f"ä¸æ”¯æŒçš„æ—¶é—´å‘¨æœŸ: {invalid_timeframes}")
        
        # è®¡ç®—é¢„æœŸè®°å½•æ•°å’Œå­ä»»åŠ¡
        start_dt = datetime.strptime(start_date, '%Y%m%d')
        end_dt = datetime.strptime(end_date, '%Y%m%d')
        
        subtasks = []
        expected_total_records = 0
        
        for symbol in symbols:
            for timeframe in timeframes:
                expected_records = self.quality_checker._calculate_expected_kline_records(
                    timeframe, start_dt, end_dt
                )
                expected_total_records += expected_records
                
                subtask = SubTaskProgress(
                    symbol=symbol,
                    timeframe=timeframe,
                    total_expected_records=expected_records,
                    status=TaskStatus.PENDING
                )
                subtasks.append(subtask)
        
        task = EnhancedDownloadTask(
            task_id=task_id,
            data_type=DataType.KLINE,
            exchange="okx",
            symbols=symbols,
            timeframes=timeframes,
            start_date=start_date,
            end_date=end_date,
            status=TaskStatus.PENDING,
            created_at=datetime.now(),
            total_files=len(symbols) * len(timeframes),
            expected_records=expected_total_records,
            subtasks=subtasks
        )
        
        self.active_tasks[task_id] = task
        await self._save_enhanced_task_to_database(task)
        
        logger.info(f"ğŸ“ åˆ›å»ºå¢å¼ºKçº¿ä»»åŠ¡: {task_id}, é¢„æœŸè®°å½•æ•°: {expected_total_records}")
        return task
    
    async def execute_enhanced_kline_task(self, task_id: str):
        """æ‰§è¡Œå¢å¼ºçš„Kçº¿ä¸‹è½½ä»»åŠ¡"""
        if task_id not in self.active_tasks:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        task = self.active_tasks[task_id]
        task.status = TaskStatus.RUNNING
        task.started_at = datetime.now()
        
        await self._update_enhanced_task_in_database(task)
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå¢å¼ºKçº¿ä»»åŠ¡: {task_id}")
            
            start_dt = datetime.strptime(task.start_date, '%Y%m%d')
            end_dt = datetime.strptime(task.end_date, '%Y%m%d')
            
            completed_subtasks = 0
            failed_subtasks = 0
            
            # é€ä¸ªå¤„ç†å­ä»»åŠ¡
            for subtask in task.subtasks:
                subtask.status = TaskStatus.RUNNING
                subtask.start_time = datetime.now()
                
                try:
                    logger.info(f"ğŸ“ˆ å¼€å§‹ä¸‹è½½å­ä»»åŠ¡: {subtask.symbol} {subtask.timeframe}")
                    
                    # ä½¿ç”¨é‡è¯•ç®¡ç†å™¨ä¸‹è½½
                    records = await self.retry_manager.execute_with_retry(
                        self._download_kline_with_validation,
                        subtask.symbol, subtask.timeframe, start_dt, end_dt, subtask
                    )
                    
                    subtask.actual_records = records
                    subtask.status = TaskStatus.COMPLETED
                    subtask.end_time = datetime.now()
                    
                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    quality_score, quality_issues = await self.quality_checker.check_kline_data_quality(
                        await self._get_db_session(), subtask.symbol, subtask.timeframe, start_dt, end_dt
                    )
                    
                    subtask.data_quality_score = quality_score
                    if quality_issues:
                        task.quality_issues.extend([f"{subtask.symbol} {subtask.timeframe}: {issue}" for issue in quality_issues])
                    
                    completed_subtasks += 1
                    task.downloaded_records += records
                    
                    logger.info(f"âœ… {subtask.symbol} {subtask.timeframe} å®Œæˆ: {records} æ¡è®°å½•, è´¨é‡è¯„åˆ†: {quality_score:.1f}")
                    
                except Exception as e:
                    subtask.status = TaskStatus.FAILED
                    subtask.error_message = str(e)
                    subtask.end_time = datetime.now()
                    task.failed_subtasks.append(subtask)
                    failed_subtasks += 1
                    
                    logger.error(f"âŒ {subtask.symbol} {subtask.timeframe} å¤±è´¥: {e}")
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                task.processed_files = completed_subtasks + failed_subtasks
                task.progress = (task.processed_files / task.total_files) * 100
                
                await self._update_enhanced_task_in_database(task)
            
            # ç¡®å®šæœ€ç»ˆçŠ¶æ€
            if failed_subtasks == 0:
                task.status = TaskStatus.COMPLETED
            elif completed_subtasks > 0:
                task.status = TaskStatus.PARTIAL_SUCCESS
                task.error_message = f"éƒ¨åˆ†æˆåŠŸ: {completed_subtasks}/{len(task.subtasks)} ä¸ªå­ä»»åŠ¡å®Œæˆ"
            else:
                task.status = TaskStatus.FAILED
                task.error_message = "æ‰€æœ‰å­ä»»åŠ¡éƒ½å¤±è´¥äº†"
            
            task.completed_at = datetime.now()
            await self._update_enhanced_task_in_database(task)
            
            logger.info(f"âœ… å¢å¼ºKçº¿ä»»åŠ¡å®Œæˆ: {task_id}, çŠ¶æ€: {task.status.value}, è®°å½•æ•°: {task.downloaded_records}")
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error_message = str(e)
            task.completed_at = datetime.now()
            await self._update_enhanced_task_in_database(task)
            logger.error(f"âŒ å¢å¼ºKçº¿ä»»åŠ¡å¤±è´¥: {task_id}, é”™è¯¯: {e}")
    
    async def _download_kline_with_validation(
        self, 
        symbol: str, 
        timeframe: str, 
        start_dt: datetime, 
        end_dt: datetime,
        subtask: SubTaskProgress
    ) -> int:
        """å¸¦éªŒè¯çš„Kçº¿æ•°æ®ä¸‹è½½"""
        try:
            # åˆ†é˜¶æ®µä¸‹è½½ï¼Œé¿å…å•æ¬¡ä¸‹è½½æ•°æ®é‡è¿‡å¤§
            total_saved = 0
            current_start = start_dt
            stage_days = 7  # æ¯æ¬¡ä¸‹è½½7å¤©çš„æ•°æ®
            
            async with AsyncSessionLocal() as db:
                while current_start < end_dt:
                    current_end = min(current_start + timedelta(days=stage_days), end_dt)
                    
                    logger.info(f"ğŸ”„ åˆ†é˜¶æ®µä¸‹è½½: {symbol} {timeframe} ({current_start.date()} åˆ° {current_end.date()})")
                    
                    # æ£€æŸ¥è¿™ä¸ªæ—¶é—´æ®µæ˜¯å¦å·²æœ‰æ•°æ®
                    existing_count = await db.execute(
                        select(func.count(MarketData.id)).where(
                            and_(
                                MarketData.exchange == "okx",
                                MarketData.symbol == symbol,
                                MarketData.timeframe == timeframe,
                                MarketData.timestamp >= current_start,
                                MarketData.timestamp <= current_end
                            )
                        )
                    )
                    existing = existing_count.scalar() or 0
                    
                    if existing > 0:
                        logger.info(f"â­ï¸ æ—¶é—´æ®µ {current_start.date()}-{current_end.date()} å·²æœ‰ {existing} æ¡æ•°æ®ï¼Œè·³è¿‡")
                        current_start = current_end
                        continue
                    
                    # ä¸‹è½½è¿™ä¸ªæ—¶é—´æ®µçš„æ•°æ®
                    stage_saved = await self._download_kline_stage(
                        symbol, timeframe, current_start, current_end, db
                    )
                    
                    total_saved += stage_saved
                    current_start = current_end
                    
                    # é¿å…APIé™æµ
                    await asyncio.sleep(1)
            
            return total_saved
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½Kçº¿æ•°æ®å¤±è´¥: {symbol} {timeframe}, é”™è¯¯: {e}")
            subtask.error_message = str(e)
            raise e
    
    async def _download_kline_stage(
        self,
        symbol: str,
        timeframe: str, 
        start_dt: datetime,
        end_dt: datetime,
        db: AsyncSession
    ) -> int:
        """ä¸‹è½½å•ä¸ªé˜¶æ®µçš„Kçº¿æ•°æ®"""
        try:
            current_end = int(end_dt.timestamp() * 1000)
            start_timestamp = int(start_dt.timestamp() * 1000)
            saved_records = 0
            request_count = 0
            
            while current_end > start_timestamp and request_count < 20:
                try:
                    # ä½¿ç”¨OKXå¸‚åœºæœåŠ¡è·å–Kçº¿æ•°æ®
                    kline_data = await okx_market_service.get_klines(
                        symbol=symbol,
                        timeframe=timeframe,
                        limit=300,
                        start_time=start_timestamp,
                        end_time=current_end,
                        use_cache=False
                    )
                    
                    if not kline_data.get('klines') or len(kline_data['klines']) == 0:
                        logger.info(f"âš ï¸ æ²¡æœ‰æ›´å¤šæ•°æ®: {symbol} {timeframe}")
                        break
                    
                    request_count += 1
                    
                    # è½¬æ¢ä¸ºMarketDataå¯¹è±¡
                    market_data_records = []
                    for kline in kline_data['klines']:
                        try:
                            timestamp_ms = int(kline[0])
                            dt_obj = dt.fromtimestamp(timestamp_ms / 1000)
                            
                            # ç¡®ä¿æ—¶é—´æˆ³åœ¨æŒ‡å®šèŒƒå›´å†…
                            if not (start_dt <= dt_obj <= end_dt):
                                continue
                            
                            # æŒ‰ç¬¦å·å†™æ³•è‡ªåŠ¨åˆ¤æ–­äº§å“ç±»å‹ï¼š*-SWAP è§†ä¸ºæ°¸ç»­(futures)ï¼Œå¦åˆ™è§†ä¸ºç°è´§(spot)
                            product_type = 'futures' if symbol.upper().endswith('-USDT-SWAP') or symbol.upper().endswith('-SWAP') else 'spot'
                            market_record = MarketData(
                                exchange="okx",
                                symbol=symbol,
                                timeframe=timeframe,
                                product_type=product_type,
                                open_price=float(kline[1]),
                                high_price=float(kline[2]),
                                low_price=float(kline[3]), 
                                close_price=float(kline[4]),
                                volume=float(kline[5]),
                                timestamp=dt_obj
                            )
                            market_data_records.append(market_record)
                        except (ValueError, IndexError) as parse_error:
                            logger.warning(f"âš ï¸ è§£æKçº¿å¤±è´¥: {kline}, é”™è¯¯: {parse_error}")
                            continue
                    
                    # æ‰¹é‡æ’å…¥å¹¶é¿å…é‡å¤
                    if market_data_records:
                        unique_records = await self._remove_duplicates_kline(db, market_data_records)
                        
                        if unique_records:
                            db.add_all(unique_records)
                            await db.commit()
                            saved_records += len(unique_records)
                            
                            logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(unique_records)} æ¡Kçº¿æ•°æ® (è·³è¿‡é‡å¤: {len(market_data_records) - len(unique_records)} æ¡)")
                    
                    # æ›´æ–°æ—¶é—´èŒƒå›´
                    if kline_data['klines']:
                        earliest_timestamp = int(kline_data['klines'][0][0])
                        
                        if earliest_timestamp <= start_timestamp:
                            logger.info(f"âœ… å·²åˆ°è¾¾èµ·å§‹æ—¶é—´")
                            break
                        
                        current_end = earliest_timestamp - 1
                    
                    # å¦‚æœè·å–çš„æ•°æ®å°‘äº300æ¡ï¼Œè¯´æ˜å·²åˆ°è¾¹ç•Œ
                    if len(kline_data['klines']) < 300:
                        logger.info(f"âœ… å·²åˆ°è¾¾æ•°æ®è¾¹ç•Œ")
                        break
                    
                    await asyncio.sleep(0.2)
                    
                except Exception as request_error:
                    logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {request_error}")
                    await asyncio.sleep(2)
                    continue
            
            logger.info(f"âœ… é˜¶æ®µä¸‹è½½å®Œæˆ: {symbol} {timeframe}, ä¿å­˜ {saved_records} æ¡è®°å½•")
            return saved_records
            
        except Exception as e:
            logger.error(f"âŒ é˜¶æ®µä¸‹è½½å¤±è´¥: {e}")
            return 0
    
    async def _get_db_session(self) -> AsyncSession:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        return AsyncSessionLocal()
    
    async def _save_enhanced_task_to_database(self, task: EnhancedDownloadTask):
        """ä¿å­˜å¢å¼ºä»»åŠ¡åˆ°æ•°æ®åº“"""
        try:
            async with AsyncSessionLocal() as db:
                # åºåˆ—åŒ–å­ä»»åŠ¡ä¿¡æ¯
                subtasks_json = json.dumps([
                    {
                        "symbol": st.symbol,
                        "timeframe": st.timeframe, 
                        "expected_records": st.total_expected_records,
                        "status": st.status.value
                    }
                    for st in task.subtasks
                ])
                
                db_task = DataCollectionTask(
                    task_name=task.task_id,
                    exchange=task.exchange,
                    data_type=task.data_type.value,
                    symbols=','.join(task.symbols),
                    timeframes=','.join(task.timeframes) if task.timeframes else None,
                    status=task.status.value,
                    schedule_type="enhanced_manual",
                    success_count=0,
                    error_count=0,
                    total_records=0,
                    created_at=task.created_at,
                    config=json.dumps({
                        "start_date": task.start_date,
                        "end_date": task.end_date,
                        "total_files": task.total_files,
                        "expected_records": task.expected_records,
                        "subtasks": subtasks_json
                    })
                )
                
                db.add(db_task)
                await db.commit()
                logger.info(f"ğŸ’¾ å¢å¼ºä»»åŠ¡å·²ä¿å­˜: {task.task_id}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¢å¼ºä»»åŠ¡å¤±è´¥: {e}")
    
    async def _update_enhanced_task_in_database(self, task: EnhancedDownloadTask):
        """æ›´æ–°å¢å¼ºä»»åŠ¡çŠ¶æ€"""
        try:
            async with AsyncSessionLocal() as db:
                result = await db.execute(
                    select(DataCollectionTask).where(
                        DataCollectionTask.task_name == task.task_id
                    )
                )
                
                db_task = result.scalar_one_or_none()
                if db_task:
                    db_task.status = task.status.value
                    db_task.success_count = task.processed_files
                    db_task.total_records = task.downloaded_records
                    db_task.last_run_at = task.started_at
                    
                    if task.completed_at:
                        db_task.next_run_at = task.completed_at
                    
                    if task.error_message:
                        db_task.last_error_message = task.error_message
                        db_task.last_error_at = datetime.now()
                    
                    # æ›´æ–°è¯¦ç»†é…ç½®ä¿¡æ¯
                    config = json.loads(db_task.config or "{}")
                    config.update({
                        "progress": task.progress,
                        "quality_issues": task.quality_issues,
                        "failed_subtasks_count": len(task.failed_subtasks),
                        "completed_subtasks": [
                            {
                                "symbol": st.symbol,
                                "timeframe": st.timeframe,
                                "records": st.actual_records,
                                "quality_score": st.data_quality_score
                            }
                            for st in task.subtasks if st.status == TaskStatus.COMPLETED
                        ]
                    })
                    db_task.config = json.dumps(config)
                    
                    await db.commit()
                    logger.debug(f"ğŸ”„ å¢å¼ºä»»åŠ¡çŠ¶æ€å·²æ›´æ–°: {task.task_id}")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å¢å¼ºä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
    
    async def _remove_duplicates_kline(self, db: AsyncSession, market_records: List[MarketData]) -> List[MarketData]:
        """ç§»é™¤é‡å¤çš„Kçº¿æ•°æ®"""
        if not market_records:
            return []
        
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            unique_keys = [(record.exchange, record.symbol, record.timeframe, record.timestamp) 
                          for record in market_records]
            
            if not unique_keys:
                return market_records
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è®°å½•
            existing_query = select(MarketData.exchange, MarketData.symbol, MarketData.timeframe, MarketData.timestamp).where(
                or_(*[
                    and_(
                        MarketData.exchange == exchange,
                        MarketData.symbol == symbol,
                        MarketData.timeframe == timeframe,
                        MarketData.timestamp == timestamp
                    ) for exchange, symbol, timeframe, timestamp in unique_keys
                ])
            )
            
            result = await db.execute(existing_query)
            existing_keys = set(result.fetchall())
            
            # è¿‡æ»¤é‡å¤è®°å½•
            unique_records = []
            for record in market_records:
                key = (record.exchange, record.symbol, record.timeframe, record.timestamp)
                if key not in existing_keys:
                    unique_records.append(record)
            
            return unique_records
            
        except Exception as e:
            logger.error(f"âŒ Kçº¿å»é‡å¤±è´¥: {e}")
            return market_records
    
    async def get_enhanced_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å¢å¼ºä»»åŠ¡çš„è¯¦ç»†çŠ¶æ€"""
        if task_id not in self.active_tasks:
            return None
        
        task = self.active_tasks[task_id]
        
        return {
            "task_id": task.task_id,
            "data_type": task.data_type.value,
            "status": task.status.value,
            "progress": task.progress,
            "symbols": task.symbols,
            "timeframes": task.timeframes,
            "expected_records": task.expected_records,
            "downloaded_records": task.downloaded_records,
            "created_at": task.created_at.isoformat() if task.created_at else None,
            "started_at": task.started_at.isoformat() if task.started_at else None,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
            "error_message": task.error_message,
            "subtasks": [
                {
                    "symbol": st.symbol,
                    "timeframe": st.timeframe,
                    "status": st.status.value,
                    "expected_records": st.total_expected_records,
                    "actual_records": st.actual_records,
                    "quality_score": st.data_quality_score,
                    "error_message": st.error_message,
                    "retry_count": st.retry_count
                }
                for st in task.subtasks
            ],
            "quality_issues": task.quality_issues,
            "failed_subtasks_count": len(task.failed_subtasks)
        }


# å…¨å±€å®ä¾‹
enhanced_okx_downloader = EnhancedOKXDataDownloader()
