"""
CCXTå†å²æ•°æ®ä¸‹è½½å™¨ - ç”Ÿäº§çº§å®ç°
æ”¯æŒé•¿æœŸå†å²æ•°æ®æ‰¹é‡ä¸‹è½½ï¼Œä¼˜åŒ–çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
"""

import ccxt
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
import time
import json
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, select
from ..database import AsyncSessionLocal
from ..models import MarketData

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CCXTHistoricalDownloader:
    """
    åŸºäºCCXTçš„å†å²æ•°æ®ä¸‹è½½å™¨
    - æ”¯æŒå¤šäº¤æ˜“æ‰€ (OKX, Binance, etc.)
    - æ™ºèƒ½åˆ†é¡µå’Œæ—¶é—´åˆ‡ç‰‡
    - ç”Ÿäº§çº§é”™è¯¯å¤„ç†å’Œé‡è¯•
    - è¿›åº¦è·Ÿè¸ªå’ŒçŠ¶æ€ç®¡ç†
    """
    
    def __init__(self, exchange_id: str = 'okx'):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            exchange_id: äº¤æ˜“æ‰€ID (okx, binance, huobi, etc.)
        """
        self.exchange_id = exchange_id
        self.exchange = self._init_exchange()
        
        # é…ç½®å‚æ•°
        self.config = {
            'rate_limit_delay': 0.11,  # 110msé—´éš”ï¼Œç¬¦åˆOKXé™åˆ¶
            'max_retries': 5,
            'retry_delay': 2,
            'batch_size': 300,  # æ¯æ¬¡è¯·æ±‚æœ€å¤§è®°å½•æ•°
            'max_concurrent': 3,  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        }
        
        # æ—¶é—´æ¡†æ¶é…ç½® 
        self.timeframe_ms = {
            '1m': 60 * 1000,
            '5m': 5 * 60 * 1000,
            '15m': 15 * 60 * 1000,
            '30m': 30 * 60 * 1000,
            '1h': 60 * 60 * 1000,
            '2h': 2 * 60 * 60 * 1000,
            '4h': 4 * 60 * 60 * 1000,
            '1d': 24 * 60 * 60 * 1000,
            '1w': 7 * 24 * 60 * 60 * 1000,
        }
        
        # çŠ¶æ€è·Ÿè¸ª
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_records': 0,
            'start_time': None,
        }
    
    def _init_exchange(self) -> ccxt.Exchange:
        """åˆå§‹åŒ–äº¤æ˜“æ‰€å®ä¾‹"""
        try:
            if self.exchange_id == 'okx':
                exchange = ccxt.okx({
                    'rateLimit': 110,  # OKXé™åˆ¶ä¸º100msï¼Œè®¾ç½®110msæ›´å®‰å…¨
                    'enableRateLimit': True,
                    'sandbox': False,
                })
            elif self.exchange_id == 'binance':
                exchange = ccxt.binance({
                    'rateLimit': 100,
                    'enableRateLimit': True,
                    'sandbox': False,
                })
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„äº¤æ˜“æ‰€: {self.exchange_id}")
            
            exchange.load_markets()
            logger.info(f"âœ… {self.exchange_id.upper()} äº¤æ˜“æ‰€åˆå§‹åŒ–æˆåŠŸ")
            return exchange
            
        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“æ‰€åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def download_historical_data(
        self,
        symbols: List[str],
        timeframes: List[str], 
        start_date: datetime,
        end_date: datetime,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½å†å²æ•°æ®çš„ä¸»æ–¹æ³•
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨ ['BTC/USDT:USDT', 'ETH/USDT:USDT']
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨ ['1m', '5m', '1h', '1d']  
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        self.stats['start_time'] = datetime.now()
        
        logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½å†å²æ•°æ®")
        logger.info(f"ğŸ“Š äº¤æ˜“æ‰€: {self.exchange_id.upper()}")
        logger.info(f"ğŸ’° äº¤æ˜“å¯¹: {symbols}")
        logger.info(f"â° æ—¶é—´æ¡†æ¶: {timeframes}")
        logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} -> {end_date}")
        
        try:
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            tasks = []
            total_tasks = len(symbols) * len(timeframes)
            completed_tasks = 0
            
            for symbol in symbols:
                for timeframe in timeframes:
                    task_info = {
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'start_date': start_date,
                        'end_date': end_date,
                        'task_id': f"{symbol}_{timeframe}",
                    }
                    tasks.append(task_info)
            
            # æ‰¹é‡å¤„ç†ä»»åŠ¡
            results = []
            semaphore = asyncio.Semaphore(self.config['max_concurrent'])
            
            async def process_task(task_info):
                async with semaphore:
                    result = await self._download_symbol_timeframe(
                        task_info['symbol'],
                        task_info['timeframe'], 
                        task_info['start_date'],
                        task_info['end_date']
                    )
                    
                    nonlocal completed_tasks
                    completed_tasks += 1
                    
                    if progress_callback:
                        progress = (completed_tasks / total_tasks) * 100
                        await progress_callback(progress, task_info, result)
                    
                    return result
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            task_coroutines = [process_task(task) for task in tasks]
            results = await asyncio.gather(*task_coroutines, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            summary = self._generate_summary(results)
            
            logger.info(f"ğŸ‰ ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ˆ æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}")
            logger.info(f"âœ… æˆåŠŸ: {self.stats['successful_requests']}")
            logger.info(f"âŒ å¤±è´¥: {self.stats['failed_requests']}")
            logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {self.stats['total_records']}")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {str(e)}")
            raise
    
    async def _download_symbol_timeframe(
        self,
        symbol: str,
        timeframe: str,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½å•ä¸ªäº¤æ˜“å¯¹å’Œæ—¶é—´æ¡†æ¶çš„æ•°æ®
        """
        logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {symbol} {timeframe}")
        
        try:
            # è½¬æ¢æ—¶é—´æˆ³
            since = int(start_date.timestamp() * 1000)
            end_timestamp = int(end_date.timestamp() * 1000)
            
            all_data = []
            current_timestamp = since
            batch_count = 0
            
            while current_timestamp < end_timestamp:
                batch_count += 1
                
                try:
                    # ä½¿ç”¨CCXTè·å–æ•°æ®
                    ohlcv_data = await self._fetch_ohlcv_batch(
                        symbol, timeframe, current_timestamp
                    )
                    
                    if not ohlcv_data:
                        logger.warning(f"âš ï¸ {symbol} {timeframe} æ‰¹æ¬¡ {batch_count} è¿”å›ç©ºæ•°æ®")
                        break
                    
                    # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
                    filtered_data = [
                        candle for candle in ohlcv_data 
                        if candle[0] <= end_timestamp
                    ]
                    
                    all_data.extend(filtered_data)
                    
                    # æ›´æ–°æ—¶é—´æˆ³
                    last_timestamp = ohlcv_data[-1][0]
                    current_timestamp = last_timestamp + self.timeframe_ms[timeframe]
                    
                    # å¦‚æœæœ€åä¸€æ ¹Kçº¿å·²ç»è¶…è¿‡ç»“æŸæ—¶é—´ï¼Œé€€å‡º
                    if last_timestamp >= end_timestamp:
                        break
                    
                    logger.info(f"ğŸ“Š {symbol} {timeframe} æ‰¹æ¬¡ {batch_count}: {len(ohlcv_data)} æ¡æ•°æ®")
                    
                    # é€Ÿç‡é™åˆ¶
                    await asyncio.sleep(self.config['rate_limit_delay'])
                    
                except Exception as e:
                    logger.error(f"âŒ {symbol} {timeframe} æ‰¹æ¬¡ {batch_count} å¤±è´¥: {str(e)}")
                    await asyncio.sleep(self.config['retry_delay'])
                    continue
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = await self._save_to_database(symbol, timeframe, all_data)
            
            result = {
                'symbol': symbol,
                'timeframe': timeframe,
                'success': True,
                'total_records': len(all_data),
                'saved_records': saved_count,
                'batches': batch_count,
                'start_time': start_date,
                'end_time': end_date,
            }
            
            logger.info(f"âœ… {symbol} {timeframe} å®Œæˆ: {saved_count} æ¡è®°å½•")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {symbol} {timeframe} ä¸‹è½½å¤±è´¥: {str(e)}")
            return {
                'symbol': symbol,
                'timeframe': timeframe,
                'success': False,
                'error': str(e),
                'total_records': 0,
                'saved_records': 0,
            }
    
    async def _fetch_ohlcv_batch(
        self, 
        symbol: str, 
        timeframe: str, 
        since: int
    ) -> List[List]:
        """
        è·å–å•æ‰¹OHLCVæ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶
        """
        for attempt in range(self.config['max_retries']):
            try:
                self.stats['total_requests'] += 1
                
                # ä½¿ç”¨CCXTçš„fetchOHLCVæ–¹æ³•
                ohlcv = self.exchange.fetch_ohlcv(
                    symbol=symbol,
                    timeframe=timeframe,
                    since=since,
                    limit=self.config['batch_size']
                )
                
                self.stats['successful_requests'] += 1
                self.stats['total_records'] += len(ohlcv)
                
                return ohlcv
                
            except ccxt.RateLimitExceeded as e:
                wait_time = self.config['retry_delay'] * (2 ** attempt)
                logger.warning(f"âš ï¸ é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}s: {str(e)}")
                await asyncio.sleep(wait_time)
                
            except (ccxt.NetworkError, ccxt.ExchangeNotAvailable) as e:
                wait_time = self.config['retry_delay'] * (2 ** attempt)
                logger.warning(f"âš ï¸ ç½‘ç»œé”™è¯¯ï¼Œé‡è¯• {attempt+1}/{self.config['max_retries']}: {str(e)}")
                if attempt < self.config['max_retries'] - 1:
                    await asyncio.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {str(e)}")
                break
        
        self.stats['failed_requests'] += 1
        return []
    
    async def _save_to_database(
        self, 
        symbol: str, 
        timeframe: str, 
        ohlcv_data: List[List]
    ) -> int:
        """
        ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
        """
        if not ohlcv_data:
            return 0
        
        try:
            async with AsyncSessionLocal() as db:
                saved_count = 0
                
                for candle in ohlcv_data:
                    timestamp, open_price, high, low, close, volume = candle
                    
                    # åˆ›å»ºMarketDataè®°å½•
                    market_data = MarketData(
                        symbol=symbol.replace('/', '').replace(':', ''),  # æ ¼å¼åŒ–äº¤æ˜“å¯¹åç§°
                        timeframe=timeframe,
                        timestamp=datetime.fromtimestamp(timestamp / 1000),
                        open_price=float(open_price),
                        high_price=float(high),
                        low_price=float(low),
                        close_price=float(close),
                        volume=float(volume),
                        exchange=self.exchange_id,
                    )
                    
                    db.add(market_data)
                    saved_count += 1
                
                await db.commit()
                return saved_count
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")
            return 0
    
    def _generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆä¸‹è½½ç»“æœæ‘˜è¦
        """
        successful = [r for r in results if isinstance(r, dict) and r.get('success', False)]
        failed = [r for r in results if isinstance(r, dict) and not r.get('success', False)]
        exceptions = [r for r in results if isinstance(r, Exception)]
        
        total_records = sum(r.get('saved_records', 0) for r in successful)
        elapsed_time = datetime.now() - self.stats['start_time']
        
        summary = {
            'success': True,
            'total_tasks': len(results),
            'successful_tasks': len(successful),
            'failed_tasks': len(failed) + len(exceptions),
            'total_records_downloaded': total_records,
            'total_api_requests': self.stats['total_requests'],
            'successful_api_requests': self.stats['successful_requests'],
            'failed_api_requests': self.stats['failed_requests'],
            'elapsed_time': str(elapsed_time),
            'download_rate': f"{total_records / elapsed_time.total_seconds():.2f} records/sec" if elapsed_time.total_seconds() > 0 else "N/A",
            'exchange': self.exchange_id,
            'timestamp': datetime.now().isoformat(),
        }
        
        return summary

# ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å‚å‡½æ•°
class CCXTDownloadTaskManager:
    """
    CCXTä¸‹è½½ä»»åŠ¡ç®¡ç†å™¨
    æä¾›ç®€åŒ–çš„æ¥å£æ¥æ‰§è¡Œæ‰¹é‡å†å²æ•°æ®ä¸‹è½½
    """
    
    @staticmethod
    async def download_okx_historical_data(
        symbols: List[str],
        timeframes: List[str],
        days_back: int = 30,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        ç®€åŒ–çš„OKXå†å²æ•°æ®ä¸‹è½½æ¥å£
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œå¦‚ ['BTC/USDT:USDT', 'ETH/USDT:USDT']  
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨ï¼Œå¦‚ ['1m', '5m', '1h', '1d']
            days_back: å‘å‰å‡ å¤©çš„æ•°æ®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ä¸‹è½½ç»“æœ
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        downloader = CCXTHistoricalDownloader('okx')
        
        return await downloader.download_historical_data(
            symbols=symbols,
            timeframes=timeframes,
            start_date=start_date,
            end_date=end_date,
            progress_callback=progress_callback
        )
    
    @staticmethod
    async def download_long_term_data(
        symbol: str,
        timeframe: str, 
        years_back: int = 1,
        exchange_id: str = 'okx'
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½é•¿æœŸå†å²æ•°æ®ï¼ˆ1-2å¹´ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹ï¼Œå¦‚ 'BTC/USDT:USDT'
            timeframe: æ—¶é—´æ¡†æ¶ï¼Œå¦‚ '1h'
            years_back: å‘å‰å‡ å¹´çš„æ•°æ®
            exchange_id: äº¤æ˜“æ‰€ID
            
        Returns:
            ä¸‹è½½ç»“æœ
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years_back * 365)
        
        downloader = CCXTHistoricalDownloader(exchange_id)
        
        return await downloader.download_historical_data(
            symbols=[symbol],
            timeframes=[timeframe],
            start_date=start_date,
            end_date=end_date,
        )