"""
Claudeæ€§èƒ½ç›‘æ§æœåŠ¡
- æ”¶é›†APIå“åº”æ—¶é—´ã€æˆåŠŸç‡ç­‰æ€§èƒ½æŒ‡æ ‡
- æ£€æµ‹å¼‚å¸¸æ¨¡å¼å’Œæˆæœ¬è¶…æ”¯
- æä¾›æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
"""

import asyncio
import time
from collections import defaultdict, deque
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import statistics
import logging
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)


class AlertLevel(str, Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning" 
    CRITICAL = "critical"


class MetricType(str, Enum):
    """æŒ‡æ ‡ç±»å‹"""
    RESPONSE_TIME = "response_time"
    SUCCESS_RATE = "success_rate"
    COST_USAGE = "cost_usage"
    REQUEST_RATE = "request_rate"
    ERROR_RATE = "error_rate"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
    timestamp: datetime
    account_id: int
    user_id: int
    metric_type: MetricType
    value: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    metric_type: MetricType
    threshold: float
    alert_level: AlertLevel
    window_seconds: int = 300  # 5åˆ†é’Ÿçª—å£
    description: str = ""


@dataclass
class PerformanceAlert:
    """æ€§èƒ½å‘Šè­¦"""
    timestamp: datetime
    alert_level: AlertLevel
    metric_type: MetricType
    account_id: int
    current_value: float
    threshold: float
    description: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class ClaudePerformanceMonitor:
    """Claudeæ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        # å†…å­˜ä¸­å­˜å‚¨æœ€è¿‘çš„æŒ‡æ ‡æ•°æ®ï¼ˆæœ€å¤šä¿å­˜1000æ¡/è´¦å·ï¼‰
        self.metrics: Dict[int, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # æ€§èƒ½ç»Ÿè®¡ç¼“å­˜
        self.stats_cache: Dict[str, Dict] = {}
        self.cache_ttl = 60  # ç¼“å­˜60ç§’
        self.last_cache_update: Dict[str, datetime] = {}
        
        # å‘Šè­¦è§„åˆ™
        self.alert_rules: List[AlertRule] = [
            AlertRule(
                metric_type=MetricType.RESPONSE_TIME,
                threshold=10.0,  # 10ç§’
                alert_level=AlertLevel.WARNING,
                description="å“åº”æ—¶é—´è¿‡é•¿"
            ),
            AlertRule(
                metric_type=MetricType.RESPONSE_TIME,
                threshold=20.0,  # 20ç§’
                alert_level=AlertLevel.CRITICAL,
                description="å“åº”æ—¶é—´ä¸¥é‡è¿‡é•¿"
            ),
            AlertRule(
                metric_type=MetricType.SUCCESS_RATE,
                threshold=0.8,  # 80%æˆåŠŸç‡
                alert_level=AlertLevel.WARNING,
                description="æˆåŠŸç‡åä½"
            ),
            AlertRule(
                metric_type=MetricType.SUCCESS_RATE,
                threshold=0.5,  # 50%æˆåŠŸç‡
                alert_level=AlertLevel.CRITICAL,
                description="æˆåŠŸç‡ä¸¥é‡åä½"
            ),
            AlertRule(
                metric_type=MetricType.COST_USAGE,
                threshold=50.0,  # $50/å¤©
                alert_level=AlertLevel.WARNING,
                description="æ—¥æˆæœ¬è¾ƒé«˜"
            ),
            AlertRule(
                metric_type=MetricType.ERROR_RATE,
                threshold=0.1,  # 10%é”™è¯¯ç‡
                alert_level=AlertLevel.WARNING,
                description="é”™è¯¯ç‡åé«˜"
            ),
        ]
        
        # æ´»è·ƒå‘Šè­¦ï¼ˆé¿å…é‡å¤å‘Šè­¦ï¼‰
        self.active_alerts: Dict[str, datetime] = {}
        self.alert_cooldown = timedelta(minutes=10)  # å‘Šè­¦å†·å´æœŸ
        
    def record_api_call(self, 
                       account_id: int,
                       user_id: int, 
                       response_time_ms: int,
                       success: bool,
                       cost_usd: float,
                       error_type: Optional[str] = None):
        """
        è®°å½•APIè°ƒç”¨æ€§èƒ½æŒ‡æ ‡
        
        Args:
            account_id: Claudeè´¦å·ID
            user_id: ç”¨æˆ·ID
            response_time_ms: å“åº”æ—¶é—´(æ¯«ç§’)
            success: æ˜¯å¦æˆåŠŸ
            cost_usd: APIæˆæœ¬(ç¾å…ƒ)
            error_type: é”™è¯¯ç±»å‹(å¦‚æœå¤±è´¥)
        """
        current_time = datetime.utcnow()
        
        # è®°å½•å“åº”æ—¶é—´
        response_time_seconds = response_time_ms / 1000.0
        self._add_metric(
            account_id, user_id, current_time,
            MetricType.RESPONSE_TIME, response_time_seconds,
            {"success": success, "error_type": error_type}
        )
        
        # è®°å½•æˆåŠŸ/å¤±è´¥çŠ¶æ€
        success_value = 1.0 if success else 0.0
        self._add_metric(
            account_id, user_id, current_time,
            MetricType.SUCCESS_RATE, success_value,
            {"error_type": error_type}
        )
        
        # è®°å½•æˆæœ¬
        self._add_metric(
            account_id, user_id, current_time,
            MetricType.COST_USAGE, cost_usd,
            {"success": success}
        )
        
        # è®°å½•é”™è¯¯ç‡
        error_value = 0.0 if success else 1.0
        self._add_metric(
            account_id, user_id, current_time,
            MetricType.ERROR_RATE, error_value,
            {"error_type": error_type}
        )
        
        # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
        asyncio.create_task(self._check_alerts(account_id))
        
    def _add_metric(self, account_id: int, user_id: int, timestamp: datetime,
                   metric_type: MetricType, value: float, metadata: Dict[str, Any]):
        """æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetric(
            timestamp=timestamp,
            account_id=account_id,
            user_id=user_id,
            metric_type=metric_type,
            value=value,
            metadata=metadata
        )
        self.metrics[account_id].append(metric)
        
        # æ¸…ç†ç¼“å­˜
        cache_key = f"{account_id}_{metric_type.value}"
        if cache_key in self.stats_cache:
            del self.stats_cache[cache_key]
        
    async def _check_alerts(self, account_id: int):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        current_time = datetime.utcnow()
        
        for rule in self.alert_rules:
            alert_key = f"{account_id}_{rule.metric_type.value}_{rule.alert_level.value}"
            
            # æ£€æŸ¥å‘Šè­¦å†·å´æœŸ
            if alert_key in self.active_alerts:
                if current_time - self.active_alerts[alert_key] < self.alert_cooldown:
                    continue
            
            # è·å–æŒ‡æ ‡æ•°æ®
            window_start = current_time - timedelta(seconds=rule.window_seconds)
            recent_metrics = [
                m for m in self.metrics[account_id] 
                if m.metric_type == rule.metric_type and m.timestamp >= window_start
            ]
            
            if not recent_metrics:
                continue
            
            # è®¡ç®—å½“å‰å€¼
            if rule.metric_type in [MetricType.SUCCESS_RATE, MetricType.ERROR_RATE]:
                current_value = statistics.mean([m.value for m in recent_metrics])
            elif rule.metric_type == MetricType.RESPONSE_TIME:
                current_value = statistics.mean([m.value for m in recent_metrics])
            elif rule.metric_type == MetricType.COST_USAGE:
                # è®¡ç®—æ—¶é—´çª—å£å†…çš„æ€»æˆæœ¬
                current_value = sum([m.value for m in recent_metrics])
            else:
                current_value = statistics.mean([m.value for m in recent_metrics])
            
            # æ£€æŸ¥é˜ˆå€¼
            trigger_alert = False
            if rule.metric_type == MetricType.SUCCESS_RATE:
                # æˆåŠŸç‡ä½äºé˜ˆå€¼
                trigger_alert = current_value < rule.threshold
            else:
                # å…¶ä»–æŒ‡æ ‡é«˜äºé˜ˆå€¼
                trigger_alert = current_value > rule.threshold
            
            if trigger_alert:
                alert = PerformanceAlert(
                    timestamp=current_time,
                    alert_level=rule.alert_level,
                    metric_type=rule.metric_type,
                    account_id=account_id,
                    current_value=current_value,
                    threshold=rule.threshold,
                    description=rule.description,
                    metadata={
                        "window_seconds": rule.window_seconds,
                        "sample_count": len(recent_metrics)
                    }
                )
                
                await self._handle_alert(alert)
                self.active_alerts[alert_key] = current_time
    
    async def _handle_alert(self, alert: PerformanceAlert):
        """å¤„ç†å‘Šè­¦"""
        log_level = logging.WARNING if alert.alert_level == AlertLevel.WARNING else logging.CRITICAL
        logger.log(
            log_level,
            f"ğŸš¨ æ€§èƒ½å‘Šè­¦: è´¦å·{alert.account_id} {alert.description} - "
            f"{alert.metric_type.value}={alert.current_value:.3f} (é˜ˆå€¼: {alert.threshold})"
        )
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šå‘Šè­¦å¤„ç†é€»è¾‘ï¼Œå¦‚å‘é€é‚®ä»¶ã€Webhookç­‰
    
    def get_account_performance_stats(self, account_id: int, 
                                    hours: int = 1) -> Dict[str, Any]:
        """
        è·å–è´¦å·æ€§èƒ½ç»Ÿè®¡
        
        Args:
            account_id: Claudeè´¦å·ID
            hours: ç»Ÿè®¡æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            åŒ…å«å„é¡¹æ€§èƒ½æŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        """
        cache_key = f"{account_id}_stats_{hours}h"
        
        # æ£€æŸ¥ç¼“å­˜
        if (cache_key in self.stats_cache and 
            cache_key in self.last_cache_update and
            (datetime.utcnow() - self.last_cache_update[cache_key]).seconds < self.cache_ttl):
            return self.stats_cache[cache_key]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        window_start = datetime.utcnow() - timedelta(hours=hours)
        account_metrics = [
            m for m in self.metrics[account_id] 
            if m.timestamp >= window_start
        ]
        
        if not account_metrics:
            stats = {
                "account_id": account_id,
                "time_window_hours": hours,
                "total_requests": 0,
                "avg_response_time": 0,
                "success_rate": 0,
                "error_rate": 0,
                "total_cost": 0,
                "requests_per_hour": 0
            }
        else:
            # åˆ†ç±»æŒ‡æ ‡
            response_times = [m.value for m in account_metrics if m.metric_type == MetricType.RESPONSE_TIME]
            success_metrics = [m.value for m in account_metrics if m.metric_type == MetricType.SUCCESS_RATE]
            error_metrics = [m.value for m in account_metrics if m.metric_type == MetricType.ERROR_RATE]
            cost_metrics = [m.value for m in account_metrics if m.metric_type == MetricType.COST_USAGE]
            
            stats = {
                "account_id": account_id,
                "time_window_hours": hours,
                "total_requests": len(response_times),
                "avg_response_time": statistics.mean(response_times) if response_times else 0,
                "median_response_time": statistics.median(response_times) if response_times else 0,
                "p95_response_time": self._percentile(response_times, 95) if response_times else 0,
                "success_rate": statistics.mean(success_metrics) if success_metrics else 0,
                "error_rate": statistics.mean(error_metrics) if error_metrics else 0,
                "total_cost": sum(cost_metrics),
                "avg_cost_per_request": statistics.mean(cost_metrics) if cost_metrics else 0,
                "requests_per_hour": len(response_times) / hours if hours > 0 else 0,
                "last_update": datetime.utcnow().isoformat()
            }
        
        # æ›´æ–°ç¼“å­˜
        self.stats_cache[cache_key] = stats
        self.last_cache_update[cache_key] = datetime.utcnow()
        
        return stats
    
    def get_system_performance_stats(self, hours: int = 1) -> Dict[str, Any]:
        """
        è·å–ç³»ç»Ÿæ•´ä½“æ€§èƒ½ç»Ÿè®¡
        
        Args:
            hours: ç»Ÿè®¡æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            ç³»ç»Ÿæ•´ä½“æ€§èƒ½æŒ‡æ ‡
        """
        cache_key = f"system_stats_{hours}h"
        
        # æ£€æŸ¥ç¼“å­˜
        if (cache_key in self.stats_cache and 
            cache_key in self.last_cache_update and
            (datetime.utcnow() - self.last_cache_update[cache_key]).seconds < self.cache_ttl):
            return self.stats_cache[cache_key]
        
        window_start = datetime.utcnow() - timedelta(hours=hours)
        
        all_response_times = []
        all_success_metrics = []
        all_error_metrics = []
        all_cost_metrics = []
        active_accounts = set()
        
        for account_id, metrics in self.metrics.items():
            account_metrics = [m for m in metrics if m.timestamp >= window_start]
            
            if account_metrics:
                active_accounts.add(account_id)
                
                for metric in account_metrics:
                    if metric.metric_type == MetricType.RESPONSE_TIME:
                        all_response_times.append(metric.value)
                    elif metric.metric_type == MetricType.SUCCESS_RATE:
                        all_success_metrics.append(metric.value)
                    elif metric.metric_type == MetricType.ERROR_RATE:
                        all_error_metrics.append(metric.value)
                    elif metric.metric_type == MetricType.COST_USAGE:
                        all_cost_metrics.append(metric.value)
        
        stats = {
            "time_window_hours": hours,
            "active_accounts": len(active_accounts),
            "total_requests": len(all_response_times),
            "avg_response_time": statistics.mean(all_response_times) if all_response_times else 0,
            "median_response_time": statistics.median(all_response_times) if all_response_times else 0,
            "p95_response_time": self._percentile(all_response_times, 95) if all_response_times else 0,
            "p99_response_time": self._percentile(all_response_times, 99) if all_response_times else 0,
            "overall_success_rate": statistics.mean(all_success_metrics) if all_success_metrics else 0,
            "overall_error_rate": statistics.mean(all_error_metrics) if all_error_metrics else 0,
            "total_cost": sum(all_cost_metrics),
            "avg_cost_per_request": statistics.mean(all_cost_metrics) if all_cost_metrics else 0,
            "requests_per_hour": len(all_response_times) / hours if hours > 0 else 0,
            "cost_per_hour": sum(all_cost_metrics) / hours if hours > 0 else 0,
            "last_update": datetime.utcnow().isoformat()
        }
        
        # æ›´æ–°ç¼“å­˜
        self.stats_cache[cache_key] = stats
        self.last_cache_update[cache_key] = datetime.utcnow()
        
        return stats
    
    def get_performance_trends(self, account_id: int, 
                              hours: int = 24, 
                              bucket_minutes: int = 60) -> Dict[str, List]:
        """
        è·å–æ€§èƒ½è¶‹åŠ¿æ•°æ®
        
        Args:
            account_id: Claudeè´¦å·ID
            hours: ç»Ÿè®¡æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            bucket_minutes: æ—¶é—´åˆ†æ¡¶å¤§å°ï¼ˆåˆ†é’Ÿï¼‰
            
        Returns:
            æ—¶é—´åºåˆ—æ€§èƒ½æ•°æ®
        """
        window_start = datetime.utcnow() - timedelta(hours=hours)
        bucket_size = timedelta(minutes=bucket_minutes)
        
        # åˆ›å»ºæ—¶é—´æ¡¶
        buckets = []
        current_bucket_start = window_start
        while current_bucket_start < datetime.utcnow():
            buckets.append(current_bucket_start)
            current_bucket_start += bucket_size
        
        # æŒ‰æ—¶é—´æ¡¶èšåˆæ•°æ®
        trend_data = {
            "timestamps": [bucket.isoformat() for bucket in buckets],
            "avg_response_time": [],
            "success_rate": [],
            "error_rate": [],
            "request_count": [],
            "total_cost": []
        }
        
        account_metrics = list(self.metrics[account_id])
        
        for bucket_start in buckets:
            bucket_end = bucket_start + bucket_size
            bucket_metrics = [
                m for m in account_metrics 
                if bucket_start <= m.timestamp < bucket_end
            ]
            
            if bucket_metrics:
                # åˆ†ç±»æŒ‡æ ‡
                response_times = [m.value for m in bucket_metrics if m.metric_type == MetricType.RESPONSE_TIME]
                success_values = [m.value for m in bucket_metrics if m.metric_type == MetricType.SUCCESS_RATE]
                error_values = [m.value for m in bucket_metrics if m.metric_type == MetricType.ERROR_RATE]
                cost_values = [m.value for m in bucket_metrics if m.metric_type == MetricType.COST_USAGE]
                
                trend_data["avg_response_time"].append(
                    statistics.mean(response_times) if response_times else 0
                )
                trend_data["success_rate"].append(
                    statistics.mean(success_values) if success_values else 0
                )
                trend_data["error_rate"].append(
                    statistics.mean(error_values) if error_values else 0
                )
                trend_data["request_count"].append(len(response_times))
                trend_data["total_cost"].append(sum(cost_values))
            else:
                # ç©ºæ¡¶
                trend_data["avg_response_time"].append(0)
                trend_data["success_rate"].append(0)
                trend_data["error_rate"].append(0)
                trend_data["request_count"].append(0)
                trend_data["total_cost"].append(0)
        
        return trend_data
    
    def get_optimization_recommendations(self, account_id: int) -> List[Dict[str, Any]]:
        """
        è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®
        
        Args:
            account_id: Claudeè´¦å·ID
            
        Returns:
            ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        stats = self.get_account_performance_stats(account_id, hours=24)
        recommendations = []
        
        # å“åº”æ—¶é—´ä¼˜åŒ–
        if stats["avg_response_time"] > 5.0:
            recommendations.append({
                "type": "performance",
                "priority": "high" if stats["avg_response_time"] > 10.0 else "medium",
                "title": "å“åº”æ—¶é—´ä¼˜åŒ–",
                "description": f"å¹³å‡å“åº”æ—¶é—´{stats['avg_response_time']:.1f}ç§’ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿæˆ–ä»£ç†é…ç½®",
                "suggestions": [
                    "æ£€æŸ¥ç½‘ç»œè¿æ¥è´¨é‡",
                    "ä¼˜åŒ–ä»£ç†æœåŠ¡å™¨é…ç½®",
                    "è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„APIç«¯ç‚¹",
                    "å®æ–½è¯·æ±‚ç¼“å­˜æœºåˆ¶"
                ]
            })
        
        # æˆåŠŸç‡ä¼˜åŒ–
        if stats["success_rate"] < 0.9:
            recommendations.append({
                "type": "reliability",
                "priority": "high" if stats["success_rate"] < 0.8 else "medium",
                "title": "å¯é æ€§æ”¹å–„",
                "description": f"æˆåŠŸç‡{stats['success_rate']:.1%}ï¼Œä½äºç†æƒ³æ°´å¹³",
                "suggestions": [
                    "æ£€æŸ¥APIå¯†é’¥æœ‰æ•ˆæ€§",
                    "å¢åŠ é‡è¯•æœºåˆ¶",
                    "å®æ–½æ–­è·¯å™¨æ¨¡å¼",
                    "ç›‘æ§è´¦å·é™é¢ä½¿ç”¨æƒ…å†µ"
                ]
            })
        
        # æˆæœ¬ä¼˜åŒ–
        if stats["avg_cost_per_request"] > 0.01:  # $0.01/è¯·æ±‚
            recommendations.append({
                "type": "cost",
                "priority": "medium",
                "title": "æˆæœ¬ä¼˜åŒ–",
                "description": f"å¹³å‡æ¯è¯·æ±‚æˆæœ¬${stats['avg_cost_per_request']:.4f}ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–",
                "suggestions": [
                    "ä¼˜åŒ–è¯·æ±‚å‚æ•°ï¼ˆå‡å°‘max_tokensï¼‰",
                    "å®æ–½æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤è¯·æ±‚",
                    "ä½¿ç”¨æˆæœ¬æ›´ä½çš„æ¨¡å‹ç‰ˆæœ¬",
                    "æ‰¹å¤„ç†ç›¸ä¼¼è¯·æ±‚"
                ]
            })
        
        # è¯·æ±‚é¢‘ç‡ä¼˜åŒ–
        if stats["requests_per_hour"] > 1000:
            recommendations.append({
                "type": "scaling",
                "priority": "low",
                "title": "æ‰©å±•æ€§ä¼˜åŒ–",
                "description": f"æ¯å°æ—¶{stats['requests_per_hour']:.0f}ä¸ªè¯·æ±‚ï¼Œå»ºè®®è€ƒè™‘è´Ÿè½½å‡è¡¡",
                "suggestions": [
                    "å¢åŠ æ›´å¤šClaudeè´¦å·",
                    "å®æ–½è¯·æ±‚è´Ÿè½½å‡è¡¡",
                    "è€ƒè™‘è¯·æ±‚é˜Ÿåˆ—æœºåˆ¶",
                    "ç›‘æ§APIé™é¢ä½¿ç”¨"
                ]
            })
        
        return recommendations
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        
        sorted_data = sorted(data)
        k = (len(sorted_data) - 1) * percentile / 100
        f = int(k)
        c = k - f
        
        if f == len(sorted_data) - 1:
            return sorted_data[f]
        else:
            return sorted_data[f] * (1 - c) + sorted_data[f + 1] * c
    
    def clear_old_metrics(self, hours: int = 48):
        """æ¸…ç†æ—§çš„æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        cleared_count = 0
        
        for account_id in self.metrics:
            old_count = len(self.metrics[account_id])
            # è¿‡æ»¤æ‰æ—§æ•°æ®
            self.metrics[account_id] = deque(
                [m for m in self.metrics[account_id] if m.timestamp > cutoff_time],
                maxlen=1000
            )
            cleared_count += old_count - len(self.metrics[account_id])
        
        if cleared_count > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†äº†{cleared_count}æ¡æ—§æ€§èƒ½æŒ‡æ ‡æ•°æ®")
        
        # æ¸…ç†ç¼“å­˜
        self.stats_cache.clear()
        self.last_cache_update.clear()


# å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹
claude_performance_monitor = ClaudePerformanceMonitor()